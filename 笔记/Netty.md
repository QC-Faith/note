# `IO`模型

5种 `IO` 模型分别是：阻塞`IO`模型、非阻塞`IO`模型、`IO`复用模型、信号驱动的`IO`模型和异步`IO`模型。

1. **阻塞IO（Blocking IO）：**

   - 在阻塞IO模型中，当用户线程发起一个IO操作（如读或写），线程会被阻塞**，转到内核空间处理**，整个IO处理完毕后返回进程，操作成功则进程获取到数据。在这个过程中，用户线程无法执行其他任务。阻塞IO模型的特点是简单易用，但可能会导致系统资源浪费。

2. **非阻塞IO（Non-blocking IO）：**

   - 当用户进程发出recvfrom调用时，如果内核中数据还没有准备好，那么并不会阻塞用户进程，而是会返回error错误。相对于用户进程而言，每次发送读取操作后，并不需要等待，而是会立刻返回结果，需要轮询不断查看状态，虽然在执行read请求操作时，用户进程并未阻塞，但是当recvfrom将数据从内核拷贝到进程时，用户进程处于阻塞状态。

3. **多路复用IO（Multiplexing IO）：**

   - 多路复用IO模型使用了操作系统提供的多路复用机制（如`select`、`poll`、`epoll`等），通过一个线程同时监听多个IO操作的状态。当其中任意一个IO操作完成时，线程会得到通知并处理。典型的例子是基于事件驱动的模型，如Reactor模式。

     > 多路复用IO又称事件驱动IO，进程使用select函数，其中select函数有一个参数是文件描述符集合，对这些文件描述符进行监听，当文件描述符就绪时，会返回readable信号，然后用户进程调用recvfrom进行读取数据，由于可同时监听多个IO,效率比阻塞IO高。
     >
     > <font color='Apricot'>**Select**</font>：进程调用select后会被阻塞，将需要监听的文件描述符放入fd_set,并将fd_set复制到内核空间，内核空间会对fd_set进行轮询遍历，若无mark值，则会暂时挂起等待超时时间之后继续轮询，直到有数据准备就绪。最后将fd_set复制回用户进程，进行读/写操作。
     >
     > > <font color='Peach'>Select的缺点</font>：
     > >
     > > 1. 每次调用select（）的时候，都必须要将fd从用户态转换成内核态，这个开销在fd很多的时候非常大。
     > > 2. 调用select（）的时候，在操作系统内核API都会轮询遍历整个fd集，这会大大影响系统效率。
     > > 3. select能监控的fd数量有上限，32位系统一般为1024，64位系统为2048
     >
     > <font color='Apricot'>**poll**</font>和select基本是一样的，但是它对fd集合做了优化，使用链表存储，解决了连接数上限的问题
     >
     > <font color='Apricot'>**Epoll**：</font>同select、poll不同，复杂度O（1），通过三个函数实现流程：
     >
     > > 1. **epoll_create**： 创建一个epoll文件描述符集合，同时底层创建一个红黑树和就绪链表，红黑树存储所监控的文件描述符的节点数据，**就绪链表存储就绪的文件描述符的节点数据**。
     > > 2. **epoll_ctl**： 用于注册要监听的事件类型，首先判断红黑树中是否存在，如果不存在，插入数据，并告知内核注册回调函数（当文件描述就绪时通过网卡驱动触发），数据就绪后将事件添加到就绪队列中。
     > > 3. **epoll_wait**： **检查链表**，并将数据拷贝到用户空间（两者维护的是片共享内存），最后清空链表。
     > >
     > > 1、对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），**会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。**
     > >
     > > 2、对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而**这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd**。
     > >
     > > 3、对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,一般来说这个数目和系统内存关系很大。
     >
     > **区别：**
     >
     > 1. select在每次被调用的时候，都会讲所有fd从用户态转换成内核态，而epoll只是在事件注册的时候只拷贝一次fd而已。提高了效率。
     > 2. 对于select来说，在每次醒着的时候，都需要将整个fd遍历一遍，而对于epoll来说，只需要在current的时候挂一遍fd，然后设置一个回调函数，当设备准备完成时，就调用一个回调函数将对应的文件描述符返还给进程，所以在时间上要大大的提高于select。
     > 3. select的文件描述符的上限默认是1024，但是epoll没有这个限制，可以远大于1024，因为它只和系统的内存大小有关，而不受限于一个定值。

4. **信号驱动IO（Signal-driven IO）：**

   - 当进程发起一个IO操作，会向内核注册一个信号处理函数，然后进程返回不阻塞；当内核数据就绪时会发送一个信号给进程，进程便在信号处理函数中调用IO读取数据。这个模型在某些平台上使用较少。

5. **异步IO（Asynchronous IO）：**

   - 当进程发起一个IO操作，进程返回(不阻塞)，但也不能返回结果。内核把整个IO处理完后，会通知进程结果，如果IO操作成功则进程直接获取到数据。异步IO模型通常需要使用操作系统提供的异步IO接口，如`aio_read`和`aio_write`。

## select、poll、epoll的机制及区别

- **单个进程打开的文件描述符（fd文件句柄）不一致**

> select ：有最大连接数限制数为1024，单个进程所能打开的最大连接数由FD_ZETSIZE宏定义。
>
> poll：poll本质上与select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的。
>
> epoll：虽然连接有上限，但是很大，1G内存的机器可以打开10万左右的连接，以此类推。

- **监听Socket的方式不一致**

> select ：轮询的方式，一个一个的socket检查过去，发现有socket活跃时才进行处理，当线性socket增多时，轮询的速度将会变得很慢，造成线性造成性能下降问题。
>
> poll：对select稍微进行了优化，只是修改了文件描述符，但是监听socket的方式还是轮询。
>
> expoll：epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，通知expoll来处理这个socket。（会将连接的socket注册到epoll中, 相当于socket的花名册, 如果有一个socket活跃了, 会回调一个函数, 通知epoll,赶紧过来处理）

- 内存空间拷贝方式（消息传递方式）不一致

> select/poll：内核想将消息传递到用户态，需要将数据从内核态拷贝到用户态,这个过程非常的耗时
>
> epoll：epoll的内核和用户空间共享一块内存，因此内存态数据和用户态数据是共享的select、poll、epoll时间复杂度分别是：O(n)、O(n)、O(1)

# JAVA的IO模型

## BIO

同步并阻塞（传统阻塞型），**服务器实现模式为一个连接一个线程**，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销。

<font color='Apricot'>对 BIO 编程流程的梳理</font>

1. 服务器端启动一个 **ServerSocket**。
2. 客户端启动 **Socket** 对服务器进行通信，默认情况下服务器端需要对每个客户建立一个线程与之通讯。
3. 客户端发出请求后，先咨询服务器是否有线程响应，如果没有则会等待，或者被拒绝。
4. 如果有响应，客户端线程会等待请求结束后，再继续执行。

<font color='Apricot'>缺陷</font>：

1. 每个请求都需要创建独立的线程，与对应的客户端进行数据 `Read`，业务处理，数据 `Write`。
2. 当并发数较大时，需要创建大量线程来处理连接，系统资源占用较大。
3. 连接建立后，如果当前线程暂时没有数据可读，则线程就阻塞在 `Read` 操作上，造成线程资源浪费。

## NIO

同步非阻塞，服务器实现模式为一个线程处理多个请求（连接），即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有 `I/O` 请求就进行处理。

NIO 是**面向缓冲区，或者面向块编程**的。数据读取到处理的缓冲区，需要时可在缓冲区中前后移动，这就增加了处理过程中的灵活性，使用它可以提供非阻塞式的高伸缩性网络。

`NIO` 有三大核心部分：`Channel`**（通道）、**`Buffer`**（缓冲区）、**`Selector`**（选择器）** 

> **Selector对应一个线程，一个线程对应多个 Channel（连接）。每个 Channel 都会对应一个 Buffer**。如果有事件发生，便获取事件然后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。只有在连接/通道真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程。避免了多线程之间的上下文切换导致的开销。
>
> 程序切换到哪个 Channel 是由**事件**决定的，Selector 会根据不同的事件，在各个通道上切换。
>
> **Buffer** **就是一个内存块，底层是有一个数组。**
>
> BIO 中要么是输入流，或者是输出流，不能双向，但是 NIO 的 Buffer 是可以读也可以写，需要 flip 方法切换
>
> **ServerSocketChannel**：在服务器端监听新的客户端 Socket 连接，负责监听，不负责实际的读写操作
>
> **SocketChannel**：网络 IO 通道，具体负责进行读写操作。NIO 把缓冲区的数据写入通道，或者把通道里的数据读到缓冲区。

<font color='Apricot'>JDK NIO 的 Bug</font>：理论上无客户端连接时Selector.select() 方法会阻塞，但空轮询bug导致：即使无客户端连接，NIO照样不断的从select本应该阻塞的Selector.select()中wake up出来，导致CPU100%问题

`Netty` 针对此问题：使用一个变量计录空轮询的数目，每空轮询一次，计数+1，当数目超过阈值512时，则重构selector，开启一个线程创建一个新的selector，遍历旧selector中所有已注册的sessionKey注册到新的selector上，最后用新selector替换旧selector。

### NIO 和 BIO 的比较

1. `BIO` 以流的方式处理数据，而 `NIO` 以块的方式处理数据，块 `I/O` 的效率比流 `I/O` 高很多。
2. `BIO` 是阻塞的，`NIO` 则是非阻塞的。
3. `BIO` 基于字节流和字符流进行操作，而 `NIO` 基于 `Channel`（通道）和 `Buffer`（缓冲区）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。`Selector`（选择器）用于监听多个通道的事件（比如：连接请求，数据到达等），因此使用单个线程就可以监听多个客户端通道。
4. Buffer和Channel之间的数据流向是双向的

## AIO

异步非阻塞，AIO 引入**异步通道**的概念，采用了 Proactor 模式（异步网络模式），它的特点是先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用。

## 使用场景

1. `BIO` 方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，`JDK1.4` 以前的唯一选择，但程序简单易理解。
2. `NIO` 方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，弹幕系统，服务器间通讯等。编程比较复杂，`JDK1.4` 开始支持。
3. `AIO` 方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用 `OS` 参与并发操作，编程比较复杂，`JDK7` 开始支持。

# Reactor 模式

**一个或多个输入同时传递给服务处理器的模式（基于事件驱动）服务器端程序处理传入的多个请求,并将它们同步分派到相应的处理线程**，因此 Reactor 模式也叫 Dispatcher 模式

Reactor 在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理线程来对 IO 事件做出反应。Handlers（处理线程EventHandler）：处理线程执行 I/O 事件要完成的实际事件

根据 Reactor 的数量和处理资源池线程的数量不同，有 3 种典型的实现

## 单 Reactor 单线程

只有一个线程，无法完全发挥多核 CPU 的性能。Handler在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈

<font color='Apricot'>使用场景</font>：客户端的数量有限，业务处理非常快速

<img src="https://gitee.com/qc_faith/picture/raw/master/image/20240312135756.png" alt="image-20240312135739539" style="zoom:67%;" />

## 单 Reactor 多线程

handler 只负责响应事件，不做具体的业务处理（这样不会使handler阻塞太久），通过 read 读取数据后，会分发给后面的 worker 线程池的某个线程处理业务。可以充分的利用多核 cpu 的处理能力。多线程数据共享和访问比较复杂。Reactor 承担所有的事件的监听和响应，它是单线程运行，在高并发场景容易出现性能瓶颈。也就是说Reactor主线程承担了过多的事

<img src="https://gitee.com/qc_faith/picture/raw/master/image/20240312135824.png" alt="image-20240312135822259" style="zoom:67%;" />

## 主从 Reactor 多线程

父线程只需要接收新连接，子线程完成后续的业务处理。父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。Nginx 主从 Reactor 多进程模型，Netty 主从多线程模型的支持

<img src="https://gitee.com/qc_faith/picture/raw/master/image/20240312135907.png" alt="image-20240312135903793" style="zoom:67%;" />

# Netty

Netty 抽象出两组线程池 ，BossGroup 专门负责接收客户端的连接，WorkerGroup 专门负责网络的读写

## 使用场景

1. 高并发、高吞吐量的网络通信应用，例如HTTP服务器、RPC框架、消息队列等。
2. 实时通信应用，例如聊天室、游戏服务器等。
3. 分布式系统中的通信组件，例如分布式缓存、分布式数据库等。
4. IoT（物联网）应用，例如智能家居、智能工厂等。
5. 在线视频流媒体应用，例如直播、点播等。

总之，任何需要高性能、高并发、低延迟的网络通信应用都可以使用Netty来实现。

## 与Java NIO的不同

**NIO有什么缺点：**

NIO的类库和API有点复杂，比如Buffer的使用 Selector编写复杂，如果对某个事件注册后，业务代码过于耦合需要了解很多多线程的知识，熟悉网络编程面对断连重连、保丢失、粘包等，处理复杂 NIO存在BUG（selector空轮训导致CPU飙升）；`Netty` 针对此问题：使用一个变量计录空轮询的数目，每空轮询一次，计数+1，当数目超过阈值512时，则重构selector，开启一个线程创建一个新的selector，遍历旧selector中所有已注册的sessionKey注册到新的selector上，最后用新selector替换旧selector。

**Netty主要的优点有：**

- 框架设计优雅，底层模型随意切换适应不同的网络协议要求
- 提供很多标准的协议、安全、编码解码的支持
- 解决了很多NIO不易用的问题
- 在很多开源框架中使用，如Dubbo、RocketMQ、Spark等
- **底层核心**：Zero-Copy-Capable Buffer，非常易用的零拷贝Buffer；统一的API；标准可扩展的时间模型
- 传输方面的支持有：管道通信；Http隧道；TCP与UDP
- 协议方面的支持有：基于原始文本和二进制的协议；解压缩；大文件传输；流媒体传输；protobuf编解码；安全认证；http和websocket

总之提供了很多现成的功能可以直接供开发者使用。

## 组件

- Channel ---- Socket

  > 基础的 IO 操作，如绑定、连接、读写等都依赖于底层网络传输所提供的原语，在 Java 的网络编程中，基础核心类是 Socket，而 Netty 的 Channel 提供了一组 API，简化了直接与 Socket 进行操作的复杂性，并且 Channel 是很多类的父类，如 EmbeddedChannel、LocalServerChannel、NioDatagramChannel、NioSctpChannel、NioSocketChannel 等。

- EventLoop ---- 控制流，多线程处理，并发；

  > EventLoop 的主要作用实际就是负责监听网络事件并调用事件处理器进行相关 IO 操作的处理。
  >
  > Channel 和 EventLoop 的联系：
  >
  > Channel 为 Netty 网络操作（读写等操作）抽象类，EventLoop 负责处理注册到其上的 Channel 处理 IO 操作，两者配合参与 IO 操作。

- ChannelFuture

  > 由于 Netty 是异步非阻塞的，所有的 IO 操作也都为异步的，我们不能立刻得到操作是否执行成功，因此 Netty 提供 ChannelFuture 接口，使用其 addListener() 方法注册一个 ChannelFutureListener，当操作执行成功或者失败时，监听就会自动触发返回结果。并且，我们还可以通过 ChannelFuture 的 channel() 方法获取关联的Channel，甚至使用 sync() 方法让异步的操作变成同步的。

- ChannelHandler和ChannelPipeline

  > ChannelHandler 存放用来处理进站和出站数据的用户逻辑。ChannelHandler 的方法被网络事件触发，可以用于几乎任何类型的操作，如将数据从一种格式转换为另一种格式或处理抛出的异常。如其子接口ChannelInboundHandler，接受进站的事件和数据以便被用户定义的逻辑处理，或者当响应所连接的客户端时刷新ChannelInboundHandler的数据。
  >
  > ChannelPipeline为ChannelHandler 链提供了一个容器并定义了用于沿着链传播入站和出站事件流的 API。当创建 Channel 时，会自动创建一个附属的 ChannelPipeline。

- Bootstrap 和 ServerBootstrap

  > Netty 的引导类应用程序网络层配置提供容器，其涉及将进程绑定到给定端口或连接一个进程到在指定主机上指定端口上运行的另一进程。引导类分为客户端引导 Bootstrap 和服务端引导 ServerBootstrap。

- 







