#事务(ACID)特性

 1. ==原子性 (`Atomicity`)==：

    是不可分割的最小操作单位，要么同时成功，要么同时失败。由在引擎层生成的<font color='Peach'>Undo log</font>来保证；每一条数据变更（insert/update/delete）操作都伴随一条undo log的生成，并且回滚日志必须先于数据持久化到磁盘上。所谓的回滚就是根据回滚日志做逆向操作

    ~~~markdown
    1. undo log，回滚日志。原子性和隔离性的MVCC就是依靠它来实现的。实现原子性的关键是当事务回滚时能够撤销所有已经成功执行的sql语句。MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改
    2. 每一条数据变更（insert/update/delete）操作都伴随一条undo log的生成，并且回滚日志必须先于数据持久化到磁盘上。所谓的回滚就是根据回滚日志做逆向操作；如果事务执行失败或调用了 rollback，导致事务需要回滚，便可以利用 undo log 中的信息将数据回滚到修改之前的样子。undo log 属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB 会根据 undo log 的内容做与之前相反的工作：
    3. 对于每个 insert，回滚时会执行 delete；
    4. 对于每个 delete，回滚时会执行insert；
    5. 对于每个 update，回滚时会执行一个相反的 update，把数据改回去。
    6. 以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。
    ~~~

 2. ==一致性 (`Consistency`)==：

    事务操作前后，数据总量不变；一致性是保证数据库在事务执行前后数据的合法性、完整性和准确性，保持一致的状态，不会因为事务的执行而导致数据不合法、不完整或不准确。

   	举例来说，如果一个银行转账的操作是一个事务，那么事务的一致性要求转账前后银行账户的总余额应该保持不变，不能因为事务的执行而出现余额不平衡或数据丢失的情况。如果事务执行过程中发生错误或异常，数据库应该回滚到事务开始前的状态，以保证数据的一致性。

 3. ==隔离性 (`Isolation`)==：

    多个事务之间。相互独立。

    数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。即事务处理过程中的中间状态对其它事务是不可见的。

    由基于悲观锁的<font color='Peach'>加锁机制</font>和基于无锁的<font color='Peach'>多版本并发控制MVCC</font>来支持；

 4. ==持久性 (`Durability`)==：

    当事务提交或回滚后，数据库会持久化的保存数据。由在引擎层生成的<font color='Peach'>Redo log（`InnoDB`存储引擎独有的）</font>和在<font color='Peach'>Service</font>层生成的<font color='Peach'>Binlog</font>来共同支撑。

    ~~~markdown
    Innnodb有很多 log，持久性靠的是 redo log。
    持久性肯定和写有关，MySQL 里经常说到的 WAL 技术(Write-Ahead Logging，预写式日志)，它的关键点就是先写日志，再写磁盘。
    
    当有一条记录要更新时，InnoDB 引擎就会先把记录写到 redo log（并更新内存），这个时候更新就算完成了。在系统比较空闲的时候，将这个操作记录更新到磁盘里面
    redo log 有两个特点：
    	大小固定，循环写
    	crash-safe
    对于redo log 是有两阶段的：commit 和 prepare 如果不使用“两阶段提交”，数据库的状态就有可能和用它的日志恢复出来的库的状态不一致. 
    
    Buffer Pool
    InnoDB还提供了缓存，Buffer Pool 中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：
    	当读取数据时，会先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；
    	当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中。
    Buffer Pool 的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时 Buffer Pool 中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。
    所以加入了 redo log。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；
    当事务提交时，会调用fsync接口对redo log进行刷盘。
    如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。
    redo log采用的是WAL（Write-Ahead Logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。而且这样做还有两个优点：
    	刷脏页是随机 IO，redo log是顺序 IO
    	刷脏页以Page为单位，一个Page上的修改整页都要写；而redo log 只包含真正需要写入的，无效 IO 减少。
    
    binlog
    bin log 也是写操作并用于数据的恢复，有啥区别呢。
    	层次：redo log 是 InnoDB 引擎特有的，server 层的叫 binlog(归档日志)
    	内容：redolog 是物理日志，记录“在某个数据页上做了什么修改”；binlog 是逻辑日志，是语句的原始逻辑，如“给 ID=2 这一行的 c 字段加 1 ”
    	写入：redolog 循环写且写入时机较多，binlog 追加且在事务提交时写入
    
    binlog 和 redo log
    	对于语句 update T set c=c+1 where ID=2;
    	1.执行器先找引擎取 ID=2 这一行。ID 是主键，直接用树搜索找到。如果 ID = 2 这一行所在数据页就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，再返回。
    	2.执行器拿到引擎给的行数据，把这个值加上 1，N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
    	3.引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
    	4.执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
    	5.执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成
    为什么先写 redo log 呢 ？
    	先 redo 后 bin : binlog 丢失，少了一次更新，恢复后仍是0。
    	先 bin 后 redo : 多了一次事务，恢复后是1。
    ~~~



# 三大日志

## Redo log

`redo log`（重做日志）是物理日志，`InnoDB`存储引擎独有，记录内容是“在某个数据页上做了什么修改”，用来实现事务持久性，主要有两部分文件组成，重做日志缓冲（redo log buffer）以及重做日志文件（redo log），前者是在内存中，后者是在磁盘中。redo log 是顺序写入 redo log file 的物理文件中去的。

`MySQL` 中数据是以页为单位，查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 `Buffer Pool` 中。后续的查询都是先从 `Buffer Pool` 中找，没有命中再去硬盘加载，减少硬盘 `IO` 开销，提升性能。

更新表数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池 Buffer Pool 上进行修改。然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（`redo log buffer`）里，接着刷盘到 `redo log` 文件里。

> 每条 redo 记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成

### 刷盘时机

1. ==log buffer空间不足时==：当 `redo log buffer` 占用的空间即将达到 `innodb_log_buffer_size` 一半的时候，后台线程会主动刷盘。

2. ==事务提交时==：

   `InnoDB` 存储引擎为 `redo log` 的刷盘策略提供了 `innodb_flush_log_at_trx_commit` 参数，它支持三种策略：

   - **0** (延迟写)：提交事务后，不会立即刷到`OS Buffer`中，而是留给后台线程每秒一次刷新到`OS Buffer`并调用`fsync()`写入`Redo Log`，因此实例崩溃将最多丢失1秒钟内的事务。

   - **1** (实时写)：每次提交事务，都会刷新到`OS Buffer`并调用`fsync()`写到`Redo Log FIle`，性能较差

   - **2** (延迟刷新)：每次提交事务只刷新到`OS Buffer`，一秒后再调用`fsync()`写入`Redo Log FIle`。因此实例崩溃不会丢失事务，宕机可能会有`1`秒数据的丢失

     `innodb_flush_log_at_trx_commit` 参数默认为 1 ，也就是说当事务提交时会调用 `fsync` 对 redo log 进行刷盘；建议在日常场景将该值设置为1，但在系统高峰期临时修改成2以应对大负载。

     <img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c49f3d879953430fbf2e8c2ba5d74db5~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?" alt="刷盘时机的总结" style="zoom: 67%;" />

3. 后台线程：

   `InnoDB` 存储引擎有一个后台线程，每隔`1` 秒就会把 `redo log buffer` 中的内容写到文件系统缓存（`page cache`），然后调用 `fsync` 刷盘。可以通过变量`innodb_flush_log_at_timeout`来控制后台线程的刷新频率

4. 正常关闭服务器时

### 文件形式

默认情况下磁盘上的 redo log 文件个数为2，每个 redo log 文件大小为 48MB，这两个 redo log 文件组成了一个日志文件组，整体是一个环形结构，从头到尾进行循环写入

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192112984.awebp" alt="日志文件组示意图" style="zoom: 67%;" />

在日志文件组中，有两个属性用于标识<font color='RedOrange'>当前写入位置</font>和<font color='RedOrange'>当前清除位置</font>，说明如下。

- <font color='RedOrange'>write pos</font>：记录当前写入位置，写入后会向后推移；
- <font color='RedOrange'>checkpoint</font>：记录当前清除位置，清除后会向后推移。

每次刷盘 `redo log` 记录到日志文件组中，`write pos` 位置就会后移更新。每次 `MySQL` 加载日志文件组恢复数据时，会清空加载过的 `redo log` 记录，并把 `checkpoint` 后移更新。

`write pos` 和 `checkpoint` 之间的还空着的部分可以用来写入新的 `redo log` 记录。如果 `write pos` 追上 `checkpoint` ，表示日志文件组满了，此时需要清除 redo log 内容以使得后续内容能够写入，具体做法是会先触发Buffer Pool 的刷盘，然后就可以清除 checkpoint 之后的部分内容（这部分内容对应 Buffer Pool 刷到磁盘上的脏页），最后 checkpoint 向后推移，也就是说 checkpoint 之前的内容其实已经被写入到了磁盘上，所以一旦MySQL宕机重启后需要根据 redo log 进行数据恢复时，只需要对 checkpoint 之后的内容进行恢复。有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

### 与直接写磁盘的区别

redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。而且这样做还有两个优点：
	刷脏页是随机 IO，redo log是顺序 IO
	刷脏页以Page为单位，大小是`16KB`，一个Page上的修改整页都要写，刷盘比较耗时；而redo log 只包含真正需要写入的（表空间号、数据页号、磁盘文件偏移量、更新值），无效 IO 减少。

## Binlog

`binlog` 是逻辑日志，以事件的形式记录了所有的`DDL`和`DML`语句(除查询语句外)，即`binlog`记录的是操作而不是数据值。不同于`redo log`的循环写入，`binlog`是追加写入，且没有固定大小限制，并不会覆盖原有日志，所以可以用来恢复到之前某个时刻的数据。也不同于`redo log`属于`InnoDB`存储引擎，`binlog`是属于`MySQL`的`Service`层，无论使用什么存储引擎，只要发生了表数据更新，都会在`Service`层记录`binlog`。

`MySQL`数据库的<font color='Peach'>数据备份、主备、主主、主从</font>都需要依靠`binlog`来同步数据，保证数据一致性。`binlog`会记录所有涉及更新数据的逻辑操作，并且是顺序写。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192133306.jpg" alt="img" style="zoom:50%;" />

### 记录格式

`binlog` 日志有三种格式，可以通过`binlog_format`参数指定。

- ==Statement==：<font color='Tasma'>记录`SQL`语句原文</font>，每一条会修改数据的 sql 都会记录在 binlog 中。同步数据时，会执行记录的 `SQL` 语句。
  - <font color='Peach'>优点</font>：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO，提高性能。(相比row能节约多少性能与日志量，这取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)
  - <font color='Peach'>缺点</font>：可能导致主从不一致，对一些系统函数不能准确复制或是不能复制( 如now()函数， last_insert_id()等)
- ==Row==：<font color='Tasma'>记录每行数据的变化</font>，保证了数据与原库一致。
  - <font color='Peach'>优点</font>：日志内容会非常清楚的记录下每一行数据被修改的细节。而且不会出现某些特定情况下存储过程或function，以及trigger的调用和触发器无法被正确复制的问题。
  - <font color='Peach'>缺点</font>：日志量太大了，特别是批量 update、整表 delete、alter 表等操作，由于要记录每一行数据的变化，此时会产生大量的日志，恢复与同步时会更消耗`IO`资源，影响执行速度。

- ==Mixedlevel==：`Statement`和`Row`的混合模式，默认采用`Statement`模式，涉及日期、函数相关的时候采用Row模式，既减少了数据量，又保证了数据一致性。

### 写入时机

当开启事务后，在事务执行过程中，会将`DDL`和`DML`的操作记录到`binlog cache`中，当提交事务时，就会将`binlog cache中`的内容先写到`page cache`，然后通过`fsync`函数将`page cache`的内容写到磁盘上的`binlog`。

`MySQL`提供了`sync_binlog`参数来控制具体的写入策略，可以通过`SHOW VARIABLES LIKE 'sync_binlog%` 语句进行查看。

- `sync_binlog` 设置为 0，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，然后由操作系统决定什么时候将`page cache`的内容写到`binlog`；这样性能得到提升，但是机器宕机，`page cache`里面的 binglog 会丢失。
- `sync_binlog` 设置为 1，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，然后调用`fsync`函数将`page cache`的内容写到`binlog`；
- `sync_binlog` 设置为 n(n > 1)，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，当向`page cache`写入数据的事务达到`n`个，此时调用`fsync`函数将`page cache`的内容写到`binlog`。在出现`IO`瓶颈的场景里，将`sync_binlog`设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近`N`个事务的`binlog`日志。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192145243.awebp" alt="写入时机示意图" style="zoom:50%;" />

### 两阶段提交

`redo log`（重做日志）让`InnoDB`存储引擎拥有了崩溃恢复能力。`binlog`（归档日志）保证了`MySQL`集群架构的数据一致性。虽然它们都属于持久化的保证，但是侧重点不同。在执行更新语句过程，会记录`redo log`与`binlog`两块日志，以基本的事务为单位，`redo log`在事务执行过程中可以不断写入，而`binlog`只有在提交事务时才写入，所以`redo log`与`binlog`的写入时机不一样。这就导致当发生`MySQL`宕机时可能会出现`redo log`和`binlog`所包含的逻辑内容不一致的问题。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192200169.jpg" alt="img" style="zoom:50%;" />

比如现在执行一条更新`SQL`语句`UPDATE student SET stu_age=22 WHERE id=1`，如果这条语句的修改写到了`redo log`中，但是在写到`binlog`前`MySQL`发生宕机，然后MySQL重启之后会根据`redo log`进行数据恢复，由于`redo log`中有更新语句的修改数据，所以这条更新语句的修改结果会写到磁盘中，但是`binlog`中是没有这条更新语句的，就会导致后续基于`binlog`进行主从同步等操作时会出现主从数据不一致的问题。

为了解决上述的问题，在`InnoDB`引擎中，使用了两阶段提交来解决。具体的实现就是将`redo log`为两个阶段，示意图如下所示。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192210479.awebp" alt="两阶段提交示意图" style="zoom:50%;" />

一条`redo log`记录可以由`事务ID + redo log记录数据 + 提交状态`组成，提交状态可以是`prepare`和`commit`，当第一次将一个数据修改写入`redo log`时，这条`redo log`记录的状态为`prepare`，这是第一阶段提交，后续提交事务时，会在`redo log`中将这个事务对应的记录的状态置为`commit`，这是第二阶段提交。根据上述的两阶段提交的写入方式，再结合`binlog`，可以在发生`MySQL`宕机导致`redo log`和`binlog`逻辑内容不一致时判断事务是否需要进行回滚。具体的判断策略如下所示。

- `binlog`无记录，`redo log`无记录。表示在第一阶段提交前发生宕机，此时需要回滚事务；
- `binlog`无记录，`redo log`为`prepare`。表示在`binlog`写完之前发生宕机，此时需要回滚事务；
- `binlog`有记录，`redo log`记录状态为`prepare`。表示在`binlog`写完之后，事务完成提交之前发生宕机，此时需要提交事务；
- `binlog`有记录，`redo log`为`commit`。表示是正常完成的事务，此时无需进行操作。

## Undo Log

`undo log`叫做**回滚日志**，属于`InnoDB`引擎。记录了某条数据变更前的旧数据，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读；当事务需要回滚时，可以通过`undo log`将数据恢复为事务修改前的数据，并且，==回滚日志会先于数据持久化到磁盘上==。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。所以`InnoDB`擎中使用`undo log`来保证了事务的原子性。

通常情况下，一条更新语句执行，写入三大日志的顺序为**undo log**>**redo log**>**binlog**。

`MVCC` 的实现依赖于：隐藏字段、Read View、==undo log==

## 日志写入顺序

==undo log > redo log > binlog==

undo:相当于数据修改前的备份

redo: 相当于数据修改后的备份，为了保证事务的持久化,redo会一直写

> 假设有A、B两个数据，值分别为1,2.
> 事务开始 ==-->== 记录A=1到undo log ==-->== 修改A=3 ==-->== 记录A=3到redo log ==-->== 记录B=2到undo log ==-->== 修改B=4 ==-->== B=4到redo log ==-->== redo log写入磁盘 ==-->== 提交事务写入bin log ==-->== 事务提交完成

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192334685.png" alt="image-20220615233733690.png" style="zoom: 33%;" />

# 死锁及解决

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

==死锁的必要条件：==

（1） 互斥条件：一个资源每次只能被一个进程使用。 

（2） 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 

（3） 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 

（4） 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。

==解除死锁的方案：==

在数据库层面，有两种策略通过 <font color='Magenta'>打破循环等待条件</font> 来解除死锁状态

1. <font color='Apricot'>设置事务等待锁的超时时间</font>：一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。
2. <font color='Apricot'>开启主动死锁检测</font>：主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

一、

> 1.查询是否锁表：show OPEN TABLES where In_use > 0;
>
> 2.查询进程（如果有SUPER权限，可以看到所有线程。否则，只能看到自己的线程）：show processlist
>
> 3.杀死进程id（就是上面命令的id列）：kill id

二、

>1：查看当前的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX;
>
>2：查看当前锁定的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS;
>
>3：查看当前等锁的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;
>
>4：杀死进程：kill 线程ID

==避免死锁的方法：==

1. 操作多张表时，尽量以相同的顺序来访问（避免形成等待环路) 

2. 避免长事务，小事务发送锁冲突的几率小。

3. 批量操作单张表数据的时候，先对数据进行排序（避免形成等待环路)

4. 尽量使用索引访问数据，避免没有where条件的操作（避免锁表）

5. 主键等值更新的时候，尽量先查询看数据库中有没有满足条件的数据，存在才更新（避免产生间隙锁）

6. 尽量使用主键更新数据，主键是唯一索引，在等值查询能查到数据的情况下只会产生行锁，不会产生间隙锁，这样产生死锁的概率就减少了。如果是范围查询，一样会产生间隙锁。

7. 在允许幻读和不可重复度的情况下，尽量使用RC的隔离级别，避免gap lock造成的死锁，因为产生死锁经常都跟间隙锁有关，间隙锁的存在本身也是在RR隔离级别来

   解决幻读的一种措施。

# Myisam和Innodb

MyISAM是MySQL5.5版之前的默认数据库引擎，虽然性能极佳，但不支持事务处理 （transaction），两者区别如下：

1. ==事务支持==

   MyISAM：强调的是性能，每次查询具有原子性，其执行数度比InnoDB类型更快，但是不提供事务支持。 

   InnoDB：支持事务，外部键等高级数据库功能。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务自动提交，宜合并事务，一同提交， 减小数据库多次提交导致的开销，提高性能。

2. ==锁差异==

   MyISAM：只支持表级锁，用户在操作 MyISAM 表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 

   InnoDB：支持事务和行级锁，是 InnoDB 的最大特色。行锁提高了多用户并发操作的性能。但是 InnoDB的行锁，只针对WHERE的条件是索引，非索引的WHERE会锁全表。

3. ==外键==

   MyISAM：不支持 InnoDB：支持

4. ==全文索引==

   MyISAM：支持(FULLTEXT类型的)全文索引 

   InnoDB：MySQL5.6 版本时 InnoDB 的版本升级到 1.2.x，此时支持全文索引

5. ==表主键==

   MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 

   InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。InnoDB的主键范围更大，最大是MyISAM的2倍。

6. ==表的具体行数==

   MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。 

   InnoDB：没有保存表的总行数(只能遍历)，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。

7. ==CURD操作==

   MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。 

   InnoDB：如果数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。 DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。

8. ==存储结构==

   MyISAM：有3个文件：Frm文件、MYD文件和MYI文件。Frm文件是表的定义文件，MYD文件是数据文件（所有的数据保存在这个文件中），MYI文件是索引文件。

   InnoDB：有2个文件：Frm文件和Ibd文件。Frm文件是表的定义文件，而Ibd文件是数据和索引存储文件（数据以主键进行聚集索引，把真正的数据保存在叶子节点中）。 InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。

   由上面可以看出，两者的区别就是<font color='Peach'>InnoDB把数据和索引都放在一个文件中，而MyISAM则是有两个文件分开存储。</font>通过索引文件和数据文件是否分离来分成聚集和非聚集索引。所以<font color='Peach'>InnoDB存储引擎是聚集索引，而MyISAM存储引擎是非聚集索引</font>。

9. ==存储空间==

   MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能 有空格，会被去掉)、动态表、压缩表。 

   InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。

10. ==可移植性、备份及恢复==

   MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针 对某个表进行操作。 

   InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。

11. ==查询效率==

    没有 where 的 count() 使用 MyISAM 要比 InnoDB 快得多。因为 MyISAM 内置了一个计数器，count() 时它直接从计数器中读，而 InnoDB 必须扫描全表。所以在 InnoDB 上执行 count() 时一般要伴随 where，且 where 中

    要包含主键以外的索引列。为什么这里特别强调“ 主键以外” ？因为InnoDB中 key primary index是和raw data 存放在一起的，而secondary index则是单独存放，然后有个指针指向primary 。所以只是count()的话

    使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回 raw data 时的作用较大。 MyISAM相对简单，所以在效率上要优于InnoDB，小型应用可以考虑使用MyISAM。


InnoDB自身很多良好的 特点，比如事务支持、存储过程、视图、行级锁定等等，在并发很多的情况下InnoDB的表现要比MyISAM强很多。

==应用场景：==

1) MyISAM：适用于读多写少的场景，管理非事务表。它提供高速存储和检索，以及全文搜索能力。

2) InnoDB：适用于要求事务支持、数据一致性和并发控制的应用用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的 INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。

# 索引

## 分类

### 以数据结构区分

1. ==B-Tree 索引（平衡树索引）：==

   MySQL 默认的索引类型

- **主要索引类型：** MySQL中常用的索引类型之一，适用于等值查询、范围查询和排序查询。
- **构成方式：** B+Tree索引按照键值的顺序构建一个平衡树，每个节点包含多个键值对，并按照一定规则分配子节点。
- **适用场景：** 适合处理范围查询，例如`WHERE column = 'value'`或`WHERE column BETWEEN 'value1' AND 'value2'`。

2. ==哈希索引：==

   Memory引擎默认支持哈希索引。MySQL 8.0 版本开始支持 InnoDB 引擎的哈希索引，但是需要通过伪Hash索引来实现，叫自适应Hash索引。

   ~~~sql
   CREATE INDEX index_name ON table_name(column_name) USING HASH;
   ~~~

- **主要特点：** 基于哈希表的索引结构，将索引列的值计算哈希值并存储在哈希表中。
- **构成方式：** 哈希索引非常快速，但是只适用于等值查询（例如`WHERE column = 'value'`），不适合范围查询。
- **适用场景：** 适合用于全值匹配的场景，但是在范围查询等其他操作下效果不佳。

3. ==全文索引（Full-Text Index）：==

   仅适用于 `MyISAM` 和 `InnoDB` 存储引擎，并且只能应用于特定类型的文本列（如 `TEXT` 和 `VARCHAR`）。

   InnoDB 从1.2.x 开始支持全文索引。MySQL5.6 版本 InnoDB 的版本升级到 1.2.x。
   
   ~~~sql
   CREATE FULLTEXT INDEX index_name ON table_name(column_name);
   ~~~

- **主要特点：** 用于全文搜索的索引类型，允许快速检索大量文本数据。
- **构成方式：** 全文索引适用于对文本字段进行全文检索，例如在`TEXT`和`VARCHAR`字段上。
- **适用场景：** 适合需要对文本数据进行关键词搜索、模糊查询的场景。

4. ==空间索引（Spatial Index）：==

   ~~~sql
   CREATE SPATIAL INDEX index_name ON table_name(column_name);
   ~~~

- **主要特点：** 用于处理空间数据类型（如`GEOMETRY`，`POINT`，`LINESTRING`等）的索引。
- **构成方式：** 空间索引支持特殊的空间算法，可以快速查询包含或相交于特定区域的数据。
- **适用场景：** 适合处理地理信息系统（GIS）、位置数据等场景。

### 按物理存储分类

MySQL索引按叶子节点存储的是否为完整表数据分为：聚簇索引（主键索引）、非聚簇索引（二级索引）

1. ==聚簇索引==
   聚簇索引就是按照每张表的主键构造一颗 B+tree，同时叶子节点中存放的就是整张表的行记录数据，聚集索引的叶子节点被称为数据页。InnoDB表要求必须有聚簇索引，默认在主键字段上建立聚簇索引，在没有主键字段的情况下，表的第一个非空的唯一索引将被建立为聚簇索引，在前两者都没有的情况下，InnoDB将自动生成一个隐式的自增id列，并在此列上建立聚簇索引。

2. ==非聚集索引（也叫二级索引、辅助索引）==

   非聚集索引的结构和聚集索引基本相同（非叶子结点存储的都是索引指针），区别在于叶子节点存放的不是行数据而是数据主键。因此在使用非聚集索引进行查找时，需要先查找到主键值，然后再到聚集索引中进行查找。

==两种索引的区别==：每个索引上包含的字段内容不同，聚集索引包含所有真实的物理数据，非聚集索引只包含索引字段和主键字段。此外，聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个。

**那非聚集索引这种查询方式算不算回表呢？**

> 回表查询简单来说就是通过非聚集索引查询数据时，得不到完整的数据内容，需要再次查询主键索引来获得数据内容。
>
> 所以如果使用非聚集索引后还需要使用其他字段的（包括在where条件中或者select子句中），则需要通过主键索引回表到聚集索引获取其他字段。
>
> 如果非聚集索引可以满足SQL语句的所有字段的，则被称为全覆盖索引，没有回表开销。
>
> 避免回表查询问题，常见的方式就是建立联合索引（组合索引），实现索引覆盖，从而避免回表查询。索引覆盖就是指索引的叶子节点已经包含了查询的数据，满足查询要求，没必要再回表进行查询。

### 按字段特性分类

MySQL索引按字段特性分类可分为：主键索引(PRIMARY KEY)、唯一索引(UNIQUE)、普通索引(INDEX)、全文索引(FULLTEXT)

1. ==主键索引(PRIMARY KEY)==
   建立在主键上的索引被称为主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。

2. ==唯一索引(UNIQUE)==
   建立在UNIQUE字段上的索引被称为唯一索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会发生重复冲突。

3. ==普通索引(INDEX)==
   建立在普通字段上的索引被称为普通索引。

4. ==全文索引(FULLTEXT)==
   MyISAM 存储引擎支持Full-text索引，用于查找文本中的关键词，而不是直接比较是否相等。Full-text索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。

   InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持Full-text索引。

### 按索引字段个数分类

MySQL索引按字段个数分类可分为：**单列索引、联合索引（也叫复合索引、组合索引）**。

1. 单列索引
   建立在单个列上的索引被称为单列索引。

2. 联合索引（复合索引、组合索引）
   建立在多个列上的索引被称为联合索引，又叫复合索引、组合索引。在MySQL中使用联合索引时要遵循**最左前缀匹配原则**。所以我们需要注意如下几个方面：

   a. 实际业务场景中创建联合索引时，我们应该把识别度比较高的字段放在前面，提高索引的命中率，充分的利用索引。

   b. 创建联合索引后，该索引的任何最左前缀都可以用于查询。比如当你有一个联合索引(col1, col2, col3)，该索引的所有最左前缀为(col1)、(col1, col2)、(col1, col2, col3)，包含这些列的所有查询都会使用该索引进行查询。

   c. 虽然联合索引可以避免回表查询，提高查询速度，但同时也会降低表数据更新的速度。因为联合索引列更新时，MySQL不仅要保存数据，还要维护一下索引文件。所以不要盲目使用，应根据业务需求来创建。

##数据结构

`MySQL `中索引的数据结构是 `B+Tree`，相比于 B-Tree 有这些优势：

> 简单概括就是`B-Tree `的非叶子结点会存放数据，而 `B+Tree` 的非叶子结点不存放数据，只存放关键字，这样同一个结点，`B+Tree `的结点能形成的叉数越多，那么存储一定数量的数据，`B+Tree` 的高度越小，那么插入、查找、删除的性能会越高。

> 通常树的插入、查找、删除的时间复杂度与树的高度成正相关，树越高，时间复杂度越高，性能越差。
>
> `MySQL `中存储数据是以页为单位的，在读数据时，也是以页为单位将数据从磁盘加载进内存当中，每从磁盘中读取一个页，就会发生一次磁盘 `IO`。也就是说，在查找数据时，每遍历一个结点，就意味着读取一个数据页，也就是发生一次` IO`。
>
> 对于` B-Tree `而言，如果要查找的数据在树的越底层，就意味着要发生的` IO `次数越多，树越高，查找时可能发生的磁盘 `IO` 次数越多，磁盘 `IO `越多，意味着程序的耗时会越长，性能越差。
>
> 而对于 `B+Tree `而言，它的树高相对 `B-Tree` 而言会低很多，而且所有的数据都是存放在叶子结点当中的，所以查询数据时，一定是遍历到叶子结点层，时间复杂度相对稳定，而且发生的磁盘 IO 次数较少，所以整体来讲，`B+Tree` 性能更优，因此 `MySQL` 的索引数据结构选择的是 `B+Tree`。
>
> 通常一颗 `B+Tree `的高度为 `3` 或者` 4`，这样一次索引的查找通常最多只需要发生 `3-4 `次磁盘 `IO`，而 `MySQL `中由于` buffer pool `缓冲池的机制，第一层的结点所对应的数据页通常存在于缓存中，第二层的数据页也有很大可能也存于缓存中（`MySQL` 的预读机制、`LRU `缓存淘汰机制等都有可能导致），因此在遍历时，`B+Tree` 的第一层、第二层极有可能不会发生磁盘` IO`，而是基于内存查找，所以一次数据查找，可能就只会发生 `1-2 `次磁盘 `IO`，这个查询效率得到了大大提高。
>
> 另外，`B+Tree` 的叶子结点中，存放了相邻叶子结点的指针，因此在进行范围查找时，不需要多次遍历整棵树，只需要找到一个叶子结点后，通过指针，就能找到其他的叶子结点，这是十分高效的一种做法。

### 为何选择B+TREE

1. B+树能显著减少IO次数，提高查询效率

   > B+ 树非叶子节点上不存储数据，仅存储键值，之所以这么做是因为在数据库中页的大小是固定的，InnoDB 中页的默认大小是 16KB。如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数又会再次减少，数据查询的效率也会更快。
   > 另外，B+ 树的阶数是等于键值的数量的，如果我们的 B+ 树一个节点可以存储 1000 个键值，那么 3 层 B+ 树可以存储 1000×1000×1000=10 亿个数据。
   > 一般根节点是常驻内存的，所以一般我们查找 10 亿数据，只需要 2 次磁盘 IO。

2. B+树的查询效率更加稳定，因为数据放在叶子节点

3. B+树能提高范围查询的效率，因为叶子节点指向下一个叶子节点

==为何不使用 B 树==

1. B树只适合随机检索，而B+树同时支持随机检索和顺序检索。

2. B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。

   > 一般来说，索引本身也很大，不可能全部存储在内存中，往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的非叶子结点只存储关键字不存储数据，内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；

3. B+树的查询效率更加稳定。

   > B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短， 只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 

4. B树元素遍历效率低下。

   > B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 

5. 增删效率更高。

   > 在 B+ 树中，所有数据都存储在叶子节点中，而非叶子节点仅存储索引。当插入或删除数据时，只需调整索引，数据不会发生移动，这降低了插入和删除操作的复杂度。

==为何不选择数组、哈希表、二叉搜索树、红黑树等数据结构作为索引？==

> MySQL 作为存储数据的组件，主要操作就是数据的**增删改查**，其中==查询==操作又是重中之重。我们经常所说的数据库优化，大部分优化的就是查询相关的操作。因此一个数据库选择何种数据结构作为索引，主要考虑因素就是这种数据结构对增删改查操作的效率如何，尤其是查询操作（通常查询操作包括等值查询、范围查询等）。

1. **数组**：不适合插入操作

   > <font color='each'>定义</font>：数组是内存中一块连续的内存空间，定义一个数组对象，这个对象的指针指向了这块内存的起始地址，如果知道数组元素的下标，就可以计算出该下标所对应的元素的内存地址了，因此可以在` O(1)`的时间复杂度内获取到元素，非常快速。
   >
   > <font color='each'>优点</font>：对于一个<font color='Magenta'>有序数组</font>，它的<font color='Magenta'>查询、删除、更新操作</font>的效率非常高。查找过程可以使用二分查找法，时间复杂度为 `O(logn)`。因为是有序数组，因此对范围查找也十分友好，只需要找到起始元素即可。如果在知道元素下标的情况下，更新操作也非常快，对于删除操作，如果我们不考虑空洞的话（如果直接将对应下标处的元素置为 null，这样这块连续内存块中相当于有个空洞），删除操作也很快。
   >
   > <font color='each'>缺点</font>：<font color='Magenta'>数组插入效率很低</font>。要往数组中间插入一个数据，需要将数组中要插入的目标位置后的所有元素先往后挪动一个位置，然后才能插入新的数据，也就是涉及到了数组的复制操作，要插入的数据越靠前，需要复制的数据就越多，不仅需要额外开辟内存，复制数据消耗的时间也很长。而在平时的开发中，生产环境的一张表的大小动辄就 1GB 以上了，如果要往中间插入一条数据时，那得复制多少数据

   因此，从插入数据这一角度来看，数组不太适合作为 MySQL 索引的数据结构。

2. **哈希表**：虽然可以快速定位，但是没有顺序，不适合范围查询，IO复杂度高。

   > <font color='each'>定义</font>：哈希表是一种 `key-value` 形式的数据结构，底层采用<font color='Magenta'>数组+链表</font>结构来实现，将 `key `通过一个哈希函数计算出一个数字，然后以该数字作为数组的下标，然后将 `value `存放到对应下标的数组中。对于不同的 `key`，在经过哈希函数计算后，可能出现相同的值(哈希冲突)，这时候就意味着同一个数组下标处要存放两个元素了，所以这个时候将数组中的元素变为一个链表，通过链表将这两个元素串联起来。
   >
   > <font color='each'>优点</font>：哈希表对于<font color='Magenta'>删除、查找、更新、插入</font>操作，都是先根据 `key `计算出一个值，就能定位到数据的目标位置了，时间复杂度都是` O(1)`，速度特别快。
   >
   > <font color='each'>缺点</font>：范围查找性能不行，例如 between...and、>=、<=等。因为哈希表的所有 `key `都会经过哈希函数计算，然后再存放数据，本来可能有序的 `key`，但经过哈希函数计算出来的值就不是有序的了，<font color='Tasma'>如果要在哈希表中进行范围查找，就只能对整个哈希表进行遍历</font>，只有符合条件范围的数据，才取出来。当我们数据库中的数据越来越多，达到几百万甚至几千万条的时候，这个时候，对整个表的遍历是非常耗时的。
   >
   > <font color='each'>适用场景</font>：哈希表适用于等值查询的场景，最经典的场景就是 `NOSQL` 数据库，例如最常用的 `Redis`，`Redis `中全是 `key-value `

3. **二叉树**：树的高度不均匀，不能自平衡，查找效率跟树的高度有关，并且IO代价高。

   > <font color='each'>定义</font>：每个节点最多有两个子结点的树称之为二叉树，比较特殊且常用的二叉树有<font color='Magenta'>二叉搜索树、`AVL` 树（平衡树）、红黑树等</font>
   >
   > <font color='each'>特点</font>：对于二叉搜索树而言，它的<font color='Magenta'>查找操作的时间复杂度就是树的高度</font>，最理想的情况下，也就是满二叉树的情况下，查找的时间复杂度为 `O(logn)`。
   >
   > <font color='each'>缺点</font>：当不停地动态地往树中插入数据、删除数据时，在极端情况下，二叉搜索树可能退化成链表，它的查找时间复杂度就变成了 `O(n)`，性能不够稳定。
   >
   > 

4. **红黑树**：树的高度随着数据量增加而增加，IO代价高。

   > <font color='each'>平衡树</font>：平衡树是在二叉查找树的基础上，增加了一条限制，左右两个子树的高度差不能超过 `1`，左右两边相对平衡。
   >
   > <font color='each'>定义</font>：在平衡二叉树的基础上又出现了红黑树，整体上来说，红黑树是一种近似平衡（不完全平衡），结点非黑即红的树，它的树高最高不会超过 `2logn`，因此查找的时间复杂度为 `O(logn)`，无论是增删改查，它的性能都十分稳定。工程上，很多地方都使用的是红黑树这种数据结构，例如 `Java` 中的` HashMap、TreeMap `等。
   >
   > <font color='each'>优点</font>：查找、删除、插入、更新的复杂度均为` O(logn)`。它的中序遍历，数据是有序的，因此也适合范围查找。
   >
   > <font color='each'>缺点</font>：为了维护平衡，它的旋转操作过于复杂；平衡树在数据动态的删除、插入过程中，为了维护平衡，避免树退化成链表，需要在<font color='Magenta'>删除或者插入数据后进行额外的旋转操作，会损耗一定的性能</font>。
   >
   > <font color='each'>总结</font>：不论是二叉搜索树、 `AVL `树，还是红黑树，都是二叉树的一种，<font color='Magenta'>每个结点最多只有两个子结点，如果存储大量数据的话，树的高度会非常高</font>。而 `MySQL `存储的数据最终是要落地到磁盘的，`MySQL `应用程序读取数据时，需要将数据从磁盘先加载到内存后才能继续操作，所以这中间会发生磁盘 `IO`，而如果树太高，每遍历一层结点时，就需要从磁盘读取一次数据，也就是发生一次 `IO`，如果数据在树高为 `20 `的地方，那查找一次数据就得发生 `20 `次 `IO`，这对应用程序简直就是灾难性的，耗时太长了。因此二叉树在 `MySQL `这种需要存储大量数据的场景下，是不适合当做索引的数据结构的，因为<font color='Magenta'>树太高，操作数据时会发生多次磁盘 `IO`，性能太差。</font>

### InnoDB一棵B+树可以存放多少行数据?

三层 B+树可存储约2千万行。

> InnoDB存储引擎最小储存单元是页，一页大小就是16k。因为B+树叶子节点存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据;
>
> 假设B+树的高度为2的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为 = 根结点指针数 * 单个叶子节点记录行数。
>
> - 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数 =16k/1k =16.
> - 非叶子节点内存放多少指针呢？我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，所以就是8+6=14字节，16k/14B =16*1024B/14B = 1170
>
> 因此，一棵高度为2的B+树，能存放1170 * 16=18720条这样的数据记录。同理一棵高度为3的B+树，能存放1170 *1170 *16 =21902400，也就是说，可以存放两千万左右的记录。B+树高度一般为1-3层，已经满足千万级别的数据存储。

## 索引优化

1. ==合理选择索引列：==

- **识别度高的列：** 对于经常用于查询的列，识别度高的列创建索引，避免对低选择性的列创建索引，以减少索引大小和提高查询效率。
- **索引字段越小越好：** 数据库的数据存储以页为单位，一页存储的数据越多一次IO操作获取的数据越大 效率越高。
- **组合索引：** 如果多个列经常一起用于查询，可以考虑创建组合索引，减少查询时的索引选择。

2. ==适当优化索引结构：==

- **适度分解索引：** 对于写入频繁的表，可能需要对索引进行适当的分解，避免索引过大造成的额外开销。例如复合索引的各列并不都在查询中被使用，某些查询可能只需要其中的一部分列。
- **尽量避免过度索引：** 过多的索引可能会导致维护开销过大和额外的存储空间消耗，需要根据实际需求和查询频率慎重选择。

3. ==合理的数据库配置：==

- **调整缓冲区大小：** 适当调整数据库的缓冲区大小，提高索引读取的效率。
- **IO优化：** 合理配置磁盘，使用SSD等快速存储设备，减少IO瓶颈对索引读写的影响。

4. ==注意查询优化：==

- **使用覆盖索引：** 尽量设计覆盖索引，即索引包含了查询所需的全部信息，避免回表操作，提高查询效率。

- **like语句的前导模糊查询不能使用索引**

  ~~~sql
  select * from doc where title like '%XX'；   --不能使用索引
  ~~~

- **union、in、or 都能够命中索引，建议使用 in**

  a. `union`能够命中索引，并且MySQL 耗费的 CPU 最少。

  ```sql
  select * from doc where status=1
  union all
  select * from doc where status=2;
  ```

  b. `in`能够命中索引，查询优化耗费的 CPU 比 `union all` 多，但可以忽略不计，一般情况下建议使用 `in`。

  ```sql
  select * from doc where status in (1, 2);
  ```

  c. `or` 新版的 MySQL 能够命中索引，查询优化耗费的 CPU 比 `in`多，不建议频繁用`or`。

  ```sql
  select * from doc where status = 1 or status = 2
  ```

  **补充**：有些地方说在`where`条件中使用`or`，索引会失效，造成全表扫描，这是个误区：

  - ①要求`where`子句使用的所有字段，都必须建立索引；
  - ②如果数据量太少，mysql制定执行计划时发现全表扫描比索引查找更快，所以会不使用索引；
  - ③确保mysql版本`5.0`以上，且查询优化器开启了`index_merge_union=on`, 也就是变量`optimizer_switch`里存在`index_merge_union`且为`on`。

- **联合索引最左前缀原则**

  a. 建立联合索引的时候，区分度最高的字段在最左边

  b. 存在非等号和等号混合判断条件时，在建立索引时，把等号条件的列前置。如 `where a>? and b=?`，那么即使`a` 的区分度更高，也必须把 `b` 放在索引的最前列。

- **不能使用索引中范围条件右边的列（范围列可以用到索引），范围列之后列的索引全失效**

  - 范围条件有：`<、<=、>、>=、between`等。

    ~~~sql
    select * from employees.titles where emp_no < 10010 and title='Senior Engineer'; -- emp_no < 10010后的条件无法使用索引
    ~~~

- **索引列上面做任何操作（计算、函数），都会导致索引失效而转向全表扫描**

  ~~~sql
  select * from order where date < = CURDATE()；
  -- 优化为具体时间
  select * from order where date < = '2018-01-2412:00:00';
  ~~~

- **强制类型转换会导致全表扫描**

- **更新十分频繁、数据区分度不高的列不宜建立索引**

  - 更新会变更 B+ 树，更新频繁的字段建立索引会大大降低数据库性能。
  - 一般区分度在80%以上的时候就可以建立索引，区分度可以使用 `count(distinct(列名))/count(*)` 来计算。

- **利用覆盖索引来进行查询操作，避免回表，减少select * 的使用**

  - 覆盖索引：查询的列和所建立的索引的列个数相同，字段相同。

- **字段描述合理使用 not null**

  - 复合索引中只要有一列含有`NULL`值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时，尽量使用`not null` 约束以及默认值。

- **IS NULL 和 IS NOT NULL 查询的情况**

  - **单列索引：** 对于单列索引，数据库通常可以利用索引来执行 `IS NULL` 和 `IS NOT NULL` 的查询，因为索引已经按照列的值排序，NULL 值和非 NULL 值可以被索引区分开来。
  - **复合索引：** 在涉及到复合索引时，如果查询条件只涉及复合索引中的一部分列或不是索引的最左侧前缀部分，可能会导致索引失效。
  - 当某个列的数据分布中 NULL 值占据了很大一部分时，使用 `IS NULL` 或 `IS NOT NULL` 查询可能不会使用索引，因为优化器可能认为全表扫描更为高效。

## 索引失效

1、如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
2、联合索引不满足最左匹配原则。
3、模糊查询时（like语句），模糊匹配的占位符位于条件的首部
4、存在索引列的数据类型隐形转换，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
5、索引列上有数学运算、函数时，索引失效
7、如果mysql估计使用全表扫描要比使用索引快，则不使用索引（比如数据量少的表）
8、避免在 where 子句中使用 != 或 <> 操作符

## 索引下推

<font color='RedOrange'>简介：</font>ICP（Index Condition Pushdown）是在MySQL 5.6版本上推出的查询优化策略，把本来由`Server`层做的索引条件检查下推给存储引擎层来做，以降低回表和访问存储引擎的次数，提高查询效率。

<font color='RedOrange'>原理：</font>

> 没有使用ICP的情况下，MySQL是如何查询的：
>
> - 存储引擎读取索引记录；
> - 根据索引中的主键值，定位并读取完整的行记录；
> - 存储引擎把记录交给`Server`层去检测该记录是否满足`WHERE`条件。
>
> 使用ICP的情况下，查询过程如下：
>
> - 读取索引记录（不是完整的行记录）；
> - 判断`WHERE`条件部分能否用索引中的列来做检查，条件不满足，则处理下一行索引记录；
> - 条件满足，使用索引中的主键去定位并读取完整的行记录（就是所谓的回表）；
> - 存储引擎把记录交给`Server`层，`Server`层检测该记录是否满足`WHERE`条件的其余部分。

<font color='RedOrange'>使用条件</font>：

- 只能用于`range`、 `ref`、 `eq_ref`、`ref_or_null`访问方法；
- 只能用于`InnoDB`和 `MyISAM`存储引擎及其分区表；
- 对`InnoDB`存储引擎来说，索引下推只适用于二级索引（也叫辅助索引）;

> 索引下推的目的是为了减少回表次数，也就是要减少IO操作。对于`InnoDB`的**聚簇索引**来说，完整的行记录已经加载到缓存区了，索引下推也就没什么意义了。

- 引用了子查询的条件不能下推；
- 引用了存储函数的条件不能下推，因为存储引擎无法调用存储函数。

# WHERE 与 HAVING

`WHERE`与`HAVING`的根本区别在于：

- `WHERE`子句在`GROUP BY`分组和聚合函数**之前**对数据行进行过滤；
- `HAVING`子句对`GROUP BY`分组和聚合函数**之后**的数据行进行过滤。
- 聚合语句(sum,min,max,avg,count)要比having子句优先执行，所有having后面可以使用聚合函数。而where子句在查询过程中执行优先级别优先于聚合语句(sum,min,max,avg,count)，所有where条件中不能使用聚合函数。

因此，`WHERE`子句中不能使用聚合函数。例如，以下语句将会返回错误：

```sql
-- 查找人数大于 5 的部门
select dept_id, count(*)
from employee
where count(*) > 5
group by dept_id;
```

由于在执行`WHERE`子句时，还没有计算聚合函数 count(*)，所以无法使用。正确的方法是使用HAVING对聚合之后的结果进行过滤：

```sql
-- 查找人数大于 5 的部门
select dept_id, count(*)
from employee
group by dept_id
having count(*) > 5;
dept_id|count(*)|
-------|--------|
      4|       9|
      5|       8|
```

另一方面，`HAVING`子句中不能使用除了分组字段和聚合函数之外的其他字段。例如，以下语句将会返回错误：

```sql
-- 统计每个部门月薪大于等于 30000 的员工人数
select dept_id, count(*)
from employee
group by dept_id
having salary >= 30000;
```

因为经过`GROUP BY`分组和聚合函数之后，不再存在 salary 字段，`HAVING`子句中只能使用分组字段或者聚合函数。

> SQLite 虽然允许`HAVING`子句中出现其他字段，但是得到的结果不正确。

从性能的角度来说，`HAVING`子句中如果使用了分组字段作为过滤条件，应该替换成`WHERE`子句；因为`WHERE`可以在执行分组操作和计算聚合函数之前过滤掉不需要的数据，性能会更好。下面示例中的语句 1 应该替换成语句 2：

```sql
-- 语句 1
select dept_id, count(*)
from employee
group by dept_id
having dept_id = 1;

-- 语句 2
select dept_id, count(*)
from employee
where dept_id = 1
group by dept_id;
```

当然，`WHERE`和`HAVING`可以组合在一起使用。例如：

```sql
select dept_id, count(*)
from employee
where salary > 10000
group by dept_id
having count(*) > 1;
dept_id|count(*)|
-------|--------|
      1|       3|
```

该语句返回了月薪大于 10000 的员工人数大于 1 的部门；`WHERE`用于过滤月薪大于 10000 的员工；`HAVING`用于过滤员工数量大于 1 的部门。



#隔离级别

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202211051835057.png" alt="img" style="zoom:67%;" />

**Read uncommitted 读未提交**   --  脏读

如果一个事务已经开始写数据，则另外一个事务不允许同时进行写操作，但允许其他事务读此行数据，该隔离级别可以通过“排他写锁”，但是==不排斥读线程实现。这样就避免了更新丢失，却可能出现脏读，也就是说事务B读取到了事务A未提交的数据==

**Read committed 读提交**   --  不可重复读

`Sql Server ,Oracle` 的默认隔离级别

如果是一个读事务，则允许其他事务读写，如果是写事务将会禁止其他事务访问该行数据，该==隔离级别避免了脏读，但是可能出现不可重复读==。事务A事先读取了数据，事务B紧接着更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。

==解决了更新丢失和脏读问题，会造成不可重复读。==

**Repeatable read(可重复读取)**   --  幻读

> ###### `MySQL`的默认隔离级别

可重复读取是指在一个事务内，多次读同一个数据，在这个事务还没结束时，其他事务不能访问该数据(包括了读写)，这样就可以在同一个事务内两次读到的数据是一样的，因此称为是可重复读隔离级别，读取数据的事务将会禁止写事务(但允许读事务)，写事务则禁止任何其他事务(包括了读写)，这样避免了不可重复读和脏读，但是有时可能会出现幻读。(读取数据的事务)可以通过“共享读镜”和“排他写锁”实现。

==解决了更新丢失、脏读、不可重复读、但是还会出现幻读==

**Serializable(可序化)**

有如下特点：

>1. <font color='RedOrange'>事务串行执行</font> ：此隔离级别下，每个事务都是串行执行的，一个事务必须等待另一个事务完成后才能开始执行。这确保了事务之间没有并发执行，从而避免了并发带来的一致性问题。
>2. <font color='RedOrange'>锁定范围广</font>：为了保证串行化执行，数据库系统需要对更多的资源进行锁定，包括行锁、表锁或其他更高级别的锁。这可能导致更多的阻塞和竞争，降低系统的并发性能。
>3. <font color='RedOrange'>一致性</font>：串行化级别提供了最高的一致性，确保事务的执行顺序与它们提交的顺序相同。这样可以避免一些其他隔离级别下可能出现的一致性问题，如幻读、不可重复读等。
>4. <font color='RedOrange'>并发性能较低</font>：由于事务需要按照串行的方式执行，串行化隔离级别通常会导致系统的并发性能较低，因为事务需要等待其他事务的释放锁才能进行。

可以避免脏读、不可重复读、幻读

==解决了更新丢失、脏读、不可重复读、幻读(虚读)==

## RR 和 RC 的区别

想要搞清楚这个问题，我们需要先弄清楚 RR 和 RC 的区别，分析下各自的优缺点。

<font color='RedOrange'>一致性读</font>

> 一致性读，又称为快照读。快照即当前行数据之前的历史版本。快照读就是使用快照信息显示基于某个时间点的查询结果，而不考虑与此同时运行的其他事务所执行的更改。
>
> 在MySQL 中，只有READ COMMITTED 和 REPEATABLE READ这两种事务隔离级别才会使用一致性读。
>
> 在 RC 中，每次读取都会重新生成一个快照，总是读取行的最新版本。
>
> 在 RR 中，快照会在事务中第一次SELECT语句执行时生成，只有在本事务中对数据进行更改才会更新快照。
>
> 在数据库的 <font color='Magenta'>RC 这种隔离级别中，还支持"半一致读"</font> ，一条update语句，如果 where 条件匹配到的记录已经加锁，那么InnoDB会返回记录最近提交的版本，由MySQL上层判断此是否需要真的加锁。

<font color='RedOrange'>锁机制</font>

> 数据库的锁，在不同的事务隔离级别下采用了不同的机制。在 MySQL 中，有三种类型的锁，分别是Record Lock、Gap Lock和 Next-Key Lock。
>
> 1. Record Lock表示记录锁，锁的是索引记录。
> 2. Gap Lock是间隙锁，锁的是索引记录之间的间隙。
> 3. Next-Key Lock是Record Lock和Gap Lock的组合，同时锁索引记录和间隙。他的范围是左开右闭的。
>
> 在 RC 中，只会对索引增加Record Lock，不会添加Gap Lock和Next-Key Lock。
>
> 在 RR 中，为了解决幻读的问题，在支持Record Lock的同时，还支持Gap Lock和Next-Key Lock；

<font color='RedOrange'>主从同步</font>

> 在数据主从同步时，不同格式的 binlog 也对事务隔离级别有要求。
>
> MySQL的binlog主要支持三种格式，分别是statement、row以及mixed，但是，RC 隔离级别只支持row格式的binlog。如果指定了mixed作为 binlog 格式，那么如果使用RC，服务器会自动使用基于row 格式的日志记录。
>
> 而 RR 的隔离级别同时支持statement、row以及mixed三种。

## 为何选 RC

1. 提升并发

   > 首先，RC 在加锁的过程中，是不需要添加Gap Lock和 Next-Key Lock 的，只对要修改的记录添加行级锁就行了。
   >
   > 这就使得并发度要比 RR 高很多。
   >
   > 另外，因为 RC 还支持"半一致读"，可以大大的减少了更新语句时行锁的冲突；对于不满足更新条件的记录，可以提前释放锁，提升并发度。

2. 减少死锁

   > 因为RR这种事务隔离级别会增加Gap Lock和 Next-Key Lock，这就使得锁的粒度变大，那么就会使得死锁的概率增大。
   >
   > 死锁：一个事务锁住了表A，然后又访问表B；另一个事务锁住了表B，然后企图访问表A；这时就会互相等待对方释放锁，就导致了死锁。

## 隔离级别与锁的关系

<font color='RedOrange'>Read Uncommitted</font> 级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 

<font color='RedOrange'>Read Committed</font> 级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁

<font color='RedOrange'>Repeatable Read</font> 级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 

<font color='RedOrange'>SERIALIZABLE</font> 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。

# MVCC

MVCC最大的优势：读不加锁，读写不冲突。在读多写少的场景下极大的增加了系统的并发性能。同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC），在某个事务对其进行操作的时候，需要查看这一条记录的隐藏列事务版本id，比对事务id并根据事物隔离级别去判断读取哪个版本的数据。

`MySQL`的`InnoDB`引擎中，**读已提交**和**可重复读**都是基于MVCC实现的，区别在于**读已提交**隔离级别下，每次查询语句执行时，均会基于当前数据库的最新状态生成快照，而**可重复读**隔离级别下，只会在第一次查询时基于当前数据库的最新状态生成快照。

==`MVCC`技术本质是依靠`undo log`和`Read View 快照`机制来实现的==。`undo log`记录了某条数据变更前的旧数据。在`MySQL`的`InnoDB`引擎中，每一条数据除了原本的字段外，还有三个隐藏字段，

​	`TRX_ID`：最后一次变更（增删改）本行数据的事务的`id`，事务每次开启前，都会从数据库获得一个**自增**长的事务ID，可以从事务ID判断事务的执行先后顺序。。

​	`roll_pointer`：指向本行数据的`undo log`的指针。

​	`ROW_ID`：（非必须）如果表中没有主键和非NULL唯一键时，则会有第三个隐藏的主键列**row_id**，用来生成默认的聚集索引。采用聚集索引的方式可以提升数据的查找效率。

变更数据时多个版本之间通过`DB_ROLL_PTR`相连并构成了一个**undo log回滚链**，以**undo log回滚链**为基础可以保证事务的**原子性**（事务回滚）和**隔离性（MVCC）**。

### Read View

- <font color='Apricot'>Read View是什么</font> ：它就是事务执行SQL语句时，产生的读视图。实际上在 InnoDB 中，每个SQL语句执行前都会得到一个Read View。
- <font color='Apricot'>Read View有什么用</font>： 它主要是用来做可见性判断的，即判断当前事务可见哪个版本的数据

快照(`Read View`)只会在**读已提交**和**可重复读**隔离级别下执行查询语句时生成；这里的**生成快照**可以理解为**决定可见的数据的范围**，快照生成完毕，即当前事务可见的数据的范围也确定了。事务生成的**快照**就是那一刻事务所能看到的数据库的状态，后续无论其它事务如何对数据库进行操作，生成**快照**的事务只能看到快照所展示的数据库的状态；

> 在==读已提交(RC==)隔离级别下，每次执行查询语句时均会生成快照;
>
> 在==可重复读(RR)==隔离级别下，只会在第一次执行查询语句时生成快照。

==`Read View`的组成内容：==

​	`m_ids`：定义`Read View`那一刻的数据库中所有未提交事务的`id`数组。

​	`min_trx_id`：`m_ids`中的最小值。

​	`max_trx_id`：表示生成`ReadView`时，系统中应该分配给下一个事务的`id`值。

​	`creator_trx_id`：创建当前`Read View`的事务的`id`。

> max_trx_id并不是m_ids中的最大值，事务id是递增分配的。比如现在有事务id为1，2，3这三个事务，之后事务id为3的事务提交了，当有一个新的事务生成ReadView时，m_ids的值就包括1和2，min_trx_id的值就是1，max_trx_id的值就是4

由于事务`id`是严格递增的，所以`Read View`可以用下图进行示意。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192254297.awebp" alt="Read View示意图" style="zoom: 67%;" />

事务生成快照时，快照中包含哪些数据（哪些数据当前事务可见），是基于**Read View**和**undo log回滚链**决定的，对于每条数据，会先从其最新版本进行判断，如果判断不可见，则根据**undo log回滚链**找到旧版本并继续判断，如果某条数据所有版本都被判断为不可见，则说明这条数据对当前事务不可见，快照中不会包含这条数据的任何版本。判断规则如下所示。

- 如果某版本的数据的<font color='RedOrange'>DB_TRX_ID < min_trx_id</font>，表明生成该版本的事务在生成`Read View`前，已经提交(因为事务ID是递增的)，故这个版本的数据对当前事务**可见**；
- 如果某版本的数据的<font color='RedOrange'>DB_TRX_ID >= max_trx_id</font>，说明最后修改这个版本的数据的事务在快照生成时还未创建，故这个版本的数据对当前事务**不可见**；
- 如果 <font color='RedOrange'>min_trx_id =< trx_id < max_trx_id</font>，分3种情况讨论：
  - （1）如果`m_ids`包含`trx_id`，则代表`Read View`生成时刻，这个事务还未提交，但是如果数据的`trx_id`等于`creator_trx_id`的话，表明数据是自己生成的，因此是**可见**的。
  - （2）如果`m_ids`包含`trx_id`，并且`trx_id`不等于`creator_trx_id`，则Read View生成时，事务未提交，并且不是自己生产的，所以当前事务**不可见**；
  - （3）如果`m_ids`不包含`trx_id`，则说明你这个事务在`Read View`生成之前就已经提交了，修改的结果，当前事务是**可见**的。

总结就是只有当前事务修改的未 commit 版本和所有已提交事务版本允许被访问。

# 主从同步

## 优势：

1. 可以实时灾备，用于故障切换；
2. 读写分离，提供查询服务，实现负载均衡；

## 实现原理

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202401041230418.png" alt="image-20240104122926268" style="zoom: 33%;" />

1. 主服务器MySQL服务将所有的写操作记录在 binlog 日志中，并生成 log dump 线程，将 binlog 日志传给从服务器MySQL服务的 I/O 线程。
2. 从服务器MySQL服务生成两个线程，一个是 I/O 线程，另一个是 SQL 线程。
3. 从库 I/O 线程去请求主库的 binlog 日志，并将 binlog 日志中的文件写入 relaylog（中继日志）中。
4. 从库的 SQL 线程会读取 relaylog 中的内容，并解析成具体的操作，来实现主从的操作一致，达到最终两个数据库数据一致的目的。

## 主从同步策略

<font color='RedOrange'>全同步复制</font>

> 客户端执行SQL时候，必须等到**所有从库**和**主库**都提交成功之后才能返回响应给客户端
>
> - 优点：可以保证数据不丢失
> - 缺点：效率很低，随着机器的增加，执行的效率会越来越低，因为要同步的机器有很多，只有等所有机器都提交之后才能返回响应给客户端

<font color='RedOrange'>异步复制</font>

> 客户端执行SQL之后，只会通知线程将binlog日志发送给从库，主库直接提交，不会等从库的响应，效率很高
>
> - 优点：效率高
> - 缺点：数据会丢失，如果同步的时候，主库提交了，但是由于一些原因导致从库并没有获取到主库发送过来的binlog日志，这时候主库挂了，那么这条数据在从库就找不到了
> - 适用场景：数据安全性不高，效率很高的场景

<font color='RedOrange'>半同步复制</font>

> 客户端执行SQL之后，至少要有一个从库同步成功才能返回响应给客户端，也就是说即使现在主库挂了，那么从库还是有这条数据
>
> 在全同步与异步取了折中的方案，<font color='Peach'>推荐此方案，可保证数据一致性</font>

<font color='RedOrange'>GTID复制</font>

>在MySql5.6之后提供了GTID复制。 GTID全程是 Global Trancation ID，是由MySql机器的唯一id+TID组成的，TID是当前MySql实例已经提交的事务数量，是逐渐递增的。
>
>Matser在更新数据之前就会生成一个GTID，一同记录到binlog中，再将binlog同步给Slave之后，Slave会先拿到GTID，然后判断这个GTID是否已经被处理过，如果被处理过就忽略，如果没有就执行
>
>优点：
>
>- 不需要再去找position进行同步了
>- 比传统复制简单

# 读写分离

读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。

因为主从复制要求slave不能写只能读 （如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时需要手动同步一下slave）。 

<font color='RedOrange'>基于*MySQL proxy* 代理的方式</font> 

><font color='Tasma'>实现方式</font>：在应用和数据库之间增加代理层，代理层接收应用对数据库的请求，根据不同请求类型转发到不同的实例，在实现读写分离的同时可以实现负载均衡。
>
><font color='Tasma'>优点</font>：直接实现读写分离和负载均衡，不用修改代码，master 和 slave 用一样的帐号，mysql 官方不建议实际生产中使用
>
><font color='Tasma'>缺点</font>：低性能， 不支持事务

<font color='RedOrange'>基于*应用内路由* 的方式</font>

><font color='Tasma'>实现方式</font>：基于spring的aop实现: 使用AbstractRoutingDataSource + aop + annotation 在dao层决定数据源。如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过 mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve 库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题，也就是不支持事务，所以我们还需要重写一下 DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。

<font color='RedOrange'>基于*MySQL* 驱动的方式</font>

> <font color='Tasma'>实现方案</font>：使用mysql驱动`Connector/J`的可以实现读写分离。即在jdbc的url中配置为如下的形示：`jdbc:mysql:replication://master,slave1,slave2,slave3/test`
>
> Java程序通过在连接MySQL的 jdbc 中配置主库与从库等地址，jdbc会自动将读请求发送给从库，将写请求发送给主库，此外，mysql的jdbc驱动还能够实现多个从库的负载均衡。

<font color='RedOrange'>基于*sharding-jdbc* 的方式</font>

> sharding-sphere是强大的读写分离、分表分库中间件，sharding-jdbc是sharding-sphere的核心模块。集成 sharding-jdbc 到项目中即可

# 执行计划

## 用法与作用

`explain + sql` ——>  可以显示优化器关于`SQL`执行的信息,`MySQL`解释了它将如何处理该语句，主要包含：

- 表的加载顺序
- `sql` 的查询类型
- 可能用到的索引，实际用到的索引
- 表与表之间的引用关系
- 一个表中有多少行被优化器查询 .....

## 包含的信息

![image-20220106114125229](https://gitee.com/qc_faith/picture/raw/master/image/image-20220106114125229.png)

执行计划包含这12个字段： `id`、`select_type`、`table`、`partitions`、`type`、`possible_keys`、`key`、`key_len`、`ref`、`rows`、`filtered`、`Extra` 。

### 1. ID

`id：` ：表示查询中执行select子句或者操作表的顺序，**`id`的值越大，代表优先级越高，越先执行**。 `id`大致会出现 3种情况：

`id`相同：具有同样的优先级，执行顺序由上而下，具体顺序由优化器决定。

`id`不同：如果 `SQL` 中存在子查询， `id`的序号会递增，`id`值越大优先级越高，越先被执行 。

`id`为`null`：最后执行

> 1. 有多少 `SELECT` 关键字，就有多少组 `id`
>
> 2. 对于**连接查询**来说，一个`SELECT`关键字后边的`FROM`子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的`id`值都是相同的
>
> 3. 对于包含**子查询**的查询语句来说，就可能涉及多个`SELECT`关键字，所以在包含子查询的查询语句的执行计划中，每个`SELECT`关键字都会对应一个唯一的`id`值
>
> 4. 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询，查看执行计划可以知道查询优化器是否对某个包含子查询的语句进行了重写
>
> 5. `id`值是`NULL`，而且`table`列名称也不是真实的表名或者表别名
>
>    `UNION`子句的作用：把多个查询的结果集合并起来并对结果集中的记录进行去重。`MySQL`使用内部的临时表对结果集去重。在内部创建了一个名为`<union1, 2>`的临时表对`id`为`1`的查询和`id`为`2`的查询的结果集合并起来并去重，所以，**id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。**
>
>    包含UNION ALL子句的查询的执行计划中，没有`id`为`NULL`的记录，UNION ALL它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，不需要为最终的结果集进行去重，所以也就不需要使用临时表。

![image-20220106152946227](https://gitee.com/qc_faith/picture/raw/master/image/image-20220106152946227.png)

### 2. select_type

表示 `select` 查询的类型，主要是用于区分各种复杂的查询，例如：`普通查询`、`联合查询`、`子查询`等。

①、SIMPLE

查询语句中不包含`UNION`或者子查询的查询都算作是`SIMPLE`类型（连接查询也算是`SIMPLE`类型）

②、PRIMARY

对于包含`UNION`、`UNION ALL`或者子查询的大查询来说，它是由几个小查询组成的，最外层 `SELECT `被标记为`PRIMARY`。

③、UNION

对于包含`UNION`、`UNION ALL`或者子查询的大查询来说，最外层 `SELECT `被标记为`PRIMARY`，其余都是`UNION`

④、UNION RESULT

`UNION RESULT`：从`union`的临时表中读取数据的 `select `被标记成 `union result` 。

⑤、SUBQUERY

如果包含子查询的查询语句不能够转为对应的`semi-join`的形式，并且该子查询是**不相关子查询**，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个`SELECT`关键

字代表的那个查询的`select_type`就是`SUBQUERY`

`select_type`为`SUBQUERY`的子查询由于会被物化，所以只需要执行一遍

> **物化：将不相关子查询的结果集写入一个临时表里而非当作外层查询的参数** 

⑥、DEPENDENT SUBQUERY

如果包含子查询的查询语句不能够转为对应的`semi-join`的形式，并且该子查询是**相关子查询**，则该子查询的第一个`SELECT`关键字代表的那个查询的`select_type`就是`DEPENDENT SUBQUERY`

`select_type`为`DEPENDENT SUBQUERY`的查询可能会被执行多次。

⑦、DEPENDENT UNION

在包含`UNION`或者`UNION ALL`的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的`select_type`的值就是`DEPENDENT UNION`。

⑧、DERIVED

对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的`select_type`就是`DERIVED`

⑨、MATERIALIZED

当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的`select_type`属性就是`MATERIALIZED`

### 3. table

查询的表名，可能是真实存在的表，有别名时显示别名，也可能为临时表

### 4. partitions

查询时匹配到的分区信息，对于非分区表值为`NULL`，当查询的是分区表时，`partitions`显示分区表命中的分区情况

### 5. type

执行计划的一条记录就代表着`MySQL`对某个表执行查询时的访问方法，其中的`type`列就表明了这个访问方法是个啥

性能从好到坏依次是：`system`  > `const` > `eq_ref` > `ref`  > `ref_or_null` > `index_merge` > `unique_subquery` > `index_subquery` > `range` > `index` > `ALL`

一般来说，得保证查询达到 `range` 级别，最好达到 `ref`

**`system`**： 当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如`MyISAM`、`Memory`，那么对该表的访问方法就是system。

**`const`**：表示查询时命中 `primary key` 主键或者 `unique` 唯一索引，或者被连接的部分是一个常量(`const`)值。这类查询速度非常快

**`eq_ref`**：在连接查询时，如果被驱动表是通过主键或者唯一索引列等值匹配的方式进行访问的（如果该主键或者唯一索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是`eq_ref`

**`ref_or_null`**：当对普通二级索引进行等值匹配查询，该索引列的值也可以是`NULL`值时，那么对该表的访问方法就可能是`ref_or_null`

**`index_merge`**：一般情况下对于某个表的查询只能使用到一个索引，单表访问方法时在某些场景下可以使用`Intersection、Union、Sort-Union`这三种索引合并的方式来执行查询

**`unique_subquery`**：类似于两表连接中被驱动表的`eq_ref`访问方法，`unique_subquery`是针对在一些包含`IN`子查询的查询语句中，如果查询优化器决定将`IN`子查询转换为`EXISTS`子查询，而且子查询可以使用到主键进行等值匹配

的话，那么该子查询执行计划的`type`列的值就是`unique_subquery`

**`index_subquery`**：`index_subquery`与`unique_subquery`类似，只不过访问子查询中的表时使用的是普通的索引

**`range`**：如果使用索引获取某些范围区间的记录，那么就可能使用到`range`访问方法

**`index`**：当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是`index`

**`ALL`**：全表扫描

> 除了`All`，其余的访问方法都能用到索引

### 6. possible_keys和key

`possible_keys`列表示在查询语句中，对某个表执行单表查询时可能用到的索引，`key`列表示实际用到的索引，查询优化器会计算使用不同索引的成本

`possible_keys`列中的值不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，尽量删除用不到的索引。

### 7. key_len

`key_len`列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的：

- 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是`VARCHAR(100)`，使用的字符集是`utf8`，那么该列实际占用的最大存储空间就是`100 × 3 = 300`个字节。

- 如果该索引列可以存储`NULL`值，则`key_len`比不可以存储`NULL`值时多1个字节。

- 对于变长字段来说，都会有`2`个字节的空间来存储该变长列的实际长度。

### 8. ref

当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是`const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery`其中之一时，`ref`列展示的就是谁在与索引列作等值匹配，比如只是一个常数或者是

某个列

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106173906914.png" alt="image-20220106173906914" style="zoom:67%;" />

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106174041253.png" alt="image-20220106174041253" style="zoom:67%;" />

### 9. rows

如果查询优化器决定使用**全表扫描**的方式对某个表执行查询时，执行计划的`rows`列就代表预计需要扫描的**行数**，如果使用**索引**来执行查询时，执行计划的`rows`列就代表预计扫描的**索引记录行数**

### 10. filtered

如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。

如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106175245267.png" alt="image-20220106175245267" style="zoom:67%;" />

### 11. Extra

此列用来说明一些额外信息

**Using index**

当查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。

**Using index condition**

有些搜索条件中虽然出现了索引列，但却不能使用到索引

**Using where**

当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息

当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。