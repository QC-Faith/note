# 事务(ACID)特性

 1. <font color='Apricot'>**原子性 (`Atomicity`)：**</font>

    是不可分割的最小操作单位，要么同时成功，要么同时失败。由在引擎层生成的<font color='Peach'>Undo log</font>来保证；每一条数据变更（insert/update/delete）操作都伴随一条undo log的生成，并且回滚日志必须先于数据持久化到磁盘上。所谓的回滚就是根据回滚日志做逆向操作

    ~~~markdown
    1. undo log，回滚日志。原子性和隔离性的MVCC就是依靠它来实现的。实现原子性的关键是当事务回滚时能够撤销所有已经成功执行的sql语句。MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改
    2. 每一条数据变更（insert/update/delete）操作都伴随一条undo log的生成，并且回滚日志必须先于数据持久化到磁盘上。所谓的回滚就是根据回滚日志做逆向操作；如果事务执行失败或调用了 rollback，导致事务需要回滚，便可以利用 undo log 中的信息将数据回滚到修改之前的样子。undo log 属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB 会根据 undo log 的内容做与之前相反的工作：
    3. 对于每个 insert，回滚时会执行 delete；
    4. 对于每个 delete，回滚时会执行insert；
    5. 对于每个 update，回滚时会执行一个相反的 update，把数据改回去。
    6. 以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。
    ~~~

 2. <font color='Chestnut Red'>**一致性 (`Consistency`)：**</font>

    事务操作前后，数据总量不变；一致性是保证数据库在事务执行前后数据的合法性、完整性和准确性，保持一致的状态，不会因为事务的执行而导致数据不合法、不完整或不准确。
    
    ~~~markdown
    举例来说，如果一个银行转账的操作是一个事务，那么事务的一致性要求转账前后银行账户的总余额应该保持不变，不能因为事务的执行而出现余额不平衡或数据丢失的情况。如果事务执行过程中发生错误或异常，数据库应该回滚到事务开始前的状态，以保证数据的一致性。
    ~~~

 3. <font color='Apricot'>**隔离性 (`Isolation`)：**</font>

    多个事务之间。相互独立。

    数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境执行。即事务处理过程中的中间状态对其它事务是不可见的。

    由基于悲观锁的<font color='Peach'>加锁机制</font>和基于无锁的<font color='Peach'>多版本并发控制MVCC</font>来支持；

 4. <font color='Apricot'>**持久性 (`Durability`)：**</font>

    当事务提交或回滚后，数据库会持久化的保存数据。由在引擎层生成的<font color='Peach'>Redo log（`InnoDB`存储引擎独有的）</font>和在<font color='Peach'>Service</font>层生成的<font color='Peach'>Binlog</font>来共同支撑。

    ~~~markdown
    Innnodb有很多 log，持久性靠的是 redo log。
    持久性肯定和写有关，MySQL 里经常说到的 WAL 技术(Write-Ahead Logging，预写式日志)，它的关键点就是先写日志，再写磁盘。
    
    当有一条记录要更新时，InnoDB 引擎就会先把记录写到 redo log（并更新内存），这个时候更新就算完成了。在系统比较空闲的时候，将这个操作记录更新到磁盘里面
    redo log 有两个特点：
    	大小固定，循环写
    	crash-safe
    对于redo log 是有两阶段的：commit 和 prepare 如果不使用“两阶段提交”，数据库的状态就有可能和用它的日志恢复出来的库的状态不一致. 
    
    Buffer Pool
    InnoDB还提供了缓存，Buffer Pool 中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：
    	当读取数据时，会先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；
    	当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中。
    Buffer Pool 的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时 Buffer Pool 中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。
    所以加入了 redo log。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；
    当事务提交时，会调用fsync接口对redo log进行刷盘。
    如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。
    redo log采用的是WAL（Write-Ahead Logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。而且这样做还有两个优点：
    	刷脏页是随机 IO，redo log是顺序 IO
    	刷脏页以Page为单位，一个Page上的修改整页都要写；而redo log 只包含真正需要写入的，无效 IO 减少。
    
    binlog
    bin log 也是写操作并用于数据的恢复，与redo log的区别如下：
    	层次：redo log 是 InnoDB 引擎特有的，binlog 属于 MySQL 的 Service 层
    	内容：redo log 是物理日志，记录“在某个数据页上做了什么修改”；binlog 是逻辑日志，是语句的原始逻辑，如“给 ID=2 这一行的 c 字段加 1 ”
    	写入：redo log 循环写且写入时机较多（Log Buffer 空间不足时、事务提交时、后台线程定期Flush），binlog 追加写且在事务提交时写入
    
    binlog 和 redo log
    	对于语句 update T set c=c+1 where ID=2;
    	1.执行器先找引擎取 ID=2 这一行。ID 是主键，直接用树搜索找到。如果 ID = 2 这一行所在数据页就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，再返回。
    	2.执行器拿到引擎给的行数据，把这个值加上 1，N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
    	3.引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
    	4.执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
    	5.执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成
    为什么先写 redo log 呢 ？
    	先 redo 后 bin : binlog 丢失，少了一次更新，恢复后仍是0。
    	先 bin 后 redo : 多了一次事务，恢复后是1。
    ~~~

## 一致性和隔离性的区别

- **一致性**关注于事务执行前后数据库的状态保持一致，一个事务执行完毕后，数据库中的数据应该仍然保持一致，不会因为事务的执行而破坏数据的完整性和约束。
- **隔离性**关注于多个事务并发执行时，它们之间的相互隔离程度，确保每个事务的执行不会相互影响，从而避免了并发执行可能带来的数据混乱和不一致性。

## 查询操作是否需要加事务

> 如果一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性； 
>
> 如果一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持。

# 三大日志

## Redo log

`redo log`（重做日志）是物理日志，`InnoDB`存储引擎独有，记录内容是“在某个数据页上做了什么修改”，用来实现事务持久性，主要有两部分文件组成，重做日志缓冲（redo log buffer）以及重做日志文件（redo log），前者是在内存中，后者是在磁盘中。redo log 是顺序写入 redo log file 的物理文件中去的。

`MySQL` 中数据是以页为单位，查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 `Buffer Pool` 中。后续的查询都是先从 `Buffer Pool` 中找，没有命中再去硬盘加载，减少硬盘 `IO` 开销，提升性能。

更新表数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池 Buffer Pool 上进行修改。然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（`redo log buffer`）里，接着刷盘到 `redo log` 文件里。

> 每条 redo 记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成

### 刷盘时机

1. <font color='RedOrange'>**Log Buffer 空间不足时**</font>：当 `redo log buffer` 占用的空间即将达到 `innodb_log_buffer_size` 一半的时候，后台线程会主动刷盘。

2. <font color='RedOrange'>**系统内存不足**</font>：当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。这种情况其实是常态，但一个查询涉及多个脏页的淘汰，会导致查询的响应时间明显变长

3. <font color='RedOrange'>**Redo log写满了**</font>：此时系统会停止所有的更新操作，把checkpoint往前推进，推进中的数据要刷到磁盘中。这种情况是InnoDB要尽量避免的。出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。

4. <font color='RedOrange'>**事务提交时**</font>：

   `InnoDB` 存储引擎为 `redo log` 的刷盘策略提供了 `innodb_flush_log_at_trx_commit` 参数，它支持三种策略：

   - **0** (延迟写)：提交事务后，不会立即刷到`OS Buffer`中，而是留给后台线程每秒一次刷新到`OS Buffer`并调用`fsync()`写入`Redo Log`，因此实例崩溃将最多丢失1秒钟内的事务。

   - **1** (实时写)：每次提交事务，都会刷新到`OS Buffer`并调用`fsync()`写到`Redo Log FIle`，性能较差

   - **2** (延迟刷新)：每次提交事务只刷新到`OS Buffer`，一秒后再调用`fsync()`写入`Redo Log FIle`。因此实例崩溃不会丢失事务，宕机可能会有`1`秒数据的丢失

     `innodb_flush_log_at_trx_commit` 参数默认为 1 ，也就是说当事务提交时会调用 `fsync` 对 redo log 进行刷盘；建议在日常场景将该值设置为1，但在系统高峰期临时修改成2以应对大负载。

     ![image-20241117012117714](https://gitee.com/qc_faith/picture/raw/master/image/202411170121751.png)

5. <font color='RedOrange'>**后台线程**</font>：

   `InnoDB` 存储引擎有一个后台线程，每隔`1` 秒就会把 `redo log buffer` 中的内容写到文件系统缓存（`page cache`），然后调用 `fsync` 刷盘。可以通过变量`innodb_flush_log_at_timeout`来控制后台线程的刷新频率

6. <font color='RedOrange'>**正常关闭服务器时**</font>

### 文件形式

默认情况下磁盘上的 redo log 文件个数为2，每个 redo log 文件大小为 48MB，这两个 redo log 文件组成了一个日志文件组，整体是一个环形结构，从头到尾进行循环写入

![image-20241117012425629](https://gitee.com/qc_faith/picture/raw/master/image/202411170124664.png)

在日志文件组中，有两个属性用于标识<font color='RedOrange'>当前写入位置</font>和<font color='RedOrange'>当前清除位置</font>，说明如下。

- <font color='RedOrange'>write pos</font>：记录当前写入位置，写入后会向后推移；
- <font color='RedOrange'>checkpoint</font>：记录当前清除位置，清除后会向后推移。

每次刷盘 `redo log` 记录到日志文件组中，`write pos` 位置就会后移更新。每次 `MySQL` 加载日志文件组恢复数据时，会清空加载过的 `redo log` 记录，并把 `checkpoint` 后移更新。

`write pos` 和 `checkpoint` 之间的还空着的部分可以用来写入新的 `redo log` 记录。如果 `write pos` 追上 `checkpoint` ，表示日志文件组满了，此时需要清除 redo log 内容以使得后续内容能够写入，具体做法是会先触发Buffer Pool 的刷盘，然后就可以清除 checkpoint 之后的部分内容（这部分内容对应 Buffer Pool 刷到磁盘上的脏页），最后 checkpoint 向后推移，也就是说 checkpoint 之前的内容其实已经被写入到了磁盘上，所以一旦MySQL宕机重启后需要根据 redo log 进行数据恢复时，只需要对 checkpoint 之后的内容进行恢复。有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

### 与直接写磁盘的区别

redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。而且这样做还有两个优点：

- 刷脏页是随机 IO，redo log是顺序 IO
- 刷脏页以Page为单位，大小是`16KB`，一个Page上的修改整页都要写，刷盘比较耗时；而redo log 只包含真正需要写入的（表空间号、数据页号、磁盘文件偏移量、更新值），无效 IO 减少。

## Binlog

`binlog` 是逻辑日志，以事件的形式记录了所有的`DDL`和`DML`语句(除查询语句外)，即`binlog`记录的是操作而不是数据值。不同于`redo log`的循环写入，`binlog`是追加写入，且没有固定大小限制，并不会覆盖原有日志，所以可以用来恢复到之前某个时刻的数据。也不同于`redo log`属于`InnoDB`存储引擎，`binlog`是属于`MySQL`的`Service`层，无论使用什么存储引擎，只要发生了表数据更新，都会在`Service`层记录`binlog`。

`MySQL`数据库的<font color='Peach'>数据备份、主备、主主、主从</font>都需要依靠`binlog`来同步数据，保证数据一致性。`binlog`会记录所有涉及更新数据的逻辑操作，并且是顺序写。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192133306.jpg" alt="img" style="zoom:50%;" />

### 记录格式

`binlog` 日志有三种格式，可以通过`binlog_format`参数指定。

- <font color='Apricot'>**Statement**</font>：<font color='Tasma'>记录`SQL`语句原文</font>，每一条会修改数据的 sql 都会记录在 binlog 中。同步数据时，会执行记录的 `SQL` 语句。
  - <font color='Peach'>优点</font>：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO，提高性能。(相比row能节约多少性能与日志量，这取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)
  - <font color='Peach'>缺点</font>：可能导致主从不一致，对一些系统函数不能准确复制或是不能复制( 如now()函数， last_insert_id()等)
- <font color='Apricot'>**Row**</font>：<font color='Tasma'>记录每行数据的变化</font>，保证了数据与原库一致。
  - <font color='Peach'>优点</font>：日志内容会非常清楚的记录下每一行数据被修改的细节。而且不会出现某些特定情况下存储过程或function，以及trigger的调用和触发器无法被正确复制的问题。
  - <font color='Peach'>缺点</font>：日志量太大了，特别是批量 update、整表 delete、alter 表等操作，由于要记录每一行数据的变化，此时会产生大量的日志，恢复与同步时会更消耗`IO`资源，影响执行速度。

- <font color='Apricot'>**Mixedlevel**</font>：`Statement`和`Row`的混合模式，默认采用`Statement`模式，涉及日期、函数相关的时候采用Row模式，既减少了数据量，又保证了数据一致性。

### 写入时机

当开启事务后，在事务执行过程中，会将`DDL`和`DML`的操作记录到`binlog cache`中，当提交事务时，就会将`binlog cache中`的内容先写到`page cache`，然后通过`fsync`函数将`page cache`的内容写到磁盘上的`binlog`。

`MySQL`提供了`sync_binlog`参数来控制具体的写入策略，可以通过`SHOW VARIABLES LIKE 'sync_binlog%` 语句进行查看。

- `sync_binlog` 设置为 0，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，然后由操作系统决定什么时候将`page cache`的内容写到`binlog`；这样性能得到提升，但是机器宕机，`page cache`里面的 binglog 会丢失。
- `sync_binlog` 设置为 1，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，然后调用`fsync`函数将`page cache`的内容写到`binlog`；
- `sync_binlog` 设置为 n(n > 1)，表示每次提交事务时，会将`binlog cache`的内容写入`page cache`，当向`page cache`写入数据的事务达到`n`个，此时调用`fsync`函数将`page cache`的内容写到`binlog`。在出现`IO`瓶颈的场景里，将`sync_binlog`设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近`N`个事务的`binlog`日志。

![image-20241117012445039](https://gitee.com/qc_faith/picture/raw/master/image/202411170124422.png)

### 两阶段提交

`redo log`（重做日志）让`InnoDB`存储引擎拥有了崩溃恢复能力。`binlog`（归档日志）保证了`MySQL`集群架构的数据一致性。虽然它们都属于持久化的保证，但是侧重点不同。在执行更新语句过程，会记录`redo log`与`binlog`两块日志，以基本的事务为单位，`redo log`在事务执行过程中可以不断写入，而`binlog`只有在提交事务时才写入，所以`redo log`与`binlog`的写入时机不一样。这就导致当发生`MySQL`宕机时可能会出现`redo log`和`binlog`所包含的逻辑内容不一致的问题。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192200169.jpg" alt="img" style="zoom:50%;" />

比如现在执行一条更新`SQL`语句`UPDATE student SET stu_age=22 WHERE id=1`，如果这条语句的修改写到了`redo log`中，但是在写到`binlog`前`MySQL`发生宕机，然后MySQL重启之后会根据`redo log`进行数据恢复，由于`redo log`中有更新语句的修改数据，所以这条更新语句的修改结果会写到磁盘中，但是`binlog`中是没有这条更新语句的，就会导致后续基于`binlog`进行主从同步等操作时会出现主从数据不一致的问题。

为了解决上述的问题，在`InnoDB`引擎中，使用了两阶段提交来解决。具体的实现就是将`redo log`为两个阶段，示意图如下所示。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202411170124807.png" alt="image-20241117012459703" style="zoom:67%;" />

一条`redo log`记录可以由`事务ID + redo log记录数据 + 提交状态`组成，提交状态可以是`prepare`和`commit`，当第一次将一个数据修改写入`redo log`时，这条`redo log`记录的状态为`prepare`，这是第一阶段提交，后续提交事务时，会在`redo log`中将这个事务对应的记录的状态置为`commit`，这是第二阶段提交。根据上述的两阶段提交的写入方式，再结合`binlog`，可以在发生`MySQL`宕机导致`redo log`和`binlog`逻辑内容不一致时判断事务是否需要进行回滚。具体的判断策略如下所示。

- `binlog`无记录，`redo log`无记录。表示在第一阶段提交前发生宕机，此时需要回滚事务；
- `binlog`无记录，`redo log`为`prepare`。表示在`binlog`写完之前发生宕机，此时需要回滚事务；
- `binlog`有记录，`redo log`记录状态为`prepare`。表示在`binlog`写完之后，事务完成提交之前发生宕机，此时需要提交事务；
- `binlog`有记录，`redo log`为`commit`。表示是正常完成的事务，此时无需进行操作。

## Undo Log

`undo log`叫做**回滚日志**，属于`InnoDB`引擎。记录了某条数据变更前的旧数据，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读；当事务需要回滚时，可以通过`undo log`将数据恢复为事务修改前的数据，并且，<font color='Chestnut Red'>回滚日志会先于数据持久化到磁盘上</font>。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。所以`InnoDB`擎中使用`undo log`来保证了事务的原子性。

通常情况下，一条更新语句执行，写入三大日志的顺序为**undo log**>**redo log**>**binlog**。

`MVCC` 的实现依赖于：隐藏字段、Read View、Undo Log

## 日志写入顺序

<font color='Chestnut Red'>undo log > redo log > binlog</font>

undo:相当于数据修改前的备份

redo: 相当于数据修改后的备份，为了保证事务的持久化,redo会一直写

> 假设有A、B两个数据，值分别为1,2.
> 事务开始 --> 记录A=1到undo log --> 修改A=3 --> 记录A=3到redo log --> 记录B=2到undo log --> 修改B=4 --> B=4到redo log --> redo log写入磁盘 --> 提交事务写入bin log --> 事务提交完成

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202312192334685.png" alt="image-20220615233733690.png" style="zoom: 33%;" />

# 死锁及解决

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

<span style="background:#f9eda6;">死锁的必要条件</span>：

（1） 互斥条件：一个资源每次只能被一个进程使用。 

（2） 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 

（3） 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 

（4） 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。

<span style="background:#f9eda6;">解除死锁的方案</span>：

在数据库层面，有两种策略通过 <font color='Magenta'>打破循环等待条件</font> 来解除死锁状态

1. <font color='Apricot'>设置事务等待锁的超时时间</font>：一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。
2. <font color='Apricot'>开启主动死锁检测</font>：主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

一、

> 1.查询是否锁表：show OPEN TABLES where In_use > 0;
>
> 2.查询进程（如果有SUPER权限，可以看到所有线程。否则，只能看到自己的线程）：show processlist
>
> 3.杀死进程id（就是上面命令的id列）：kill id

二、

>1：查看当前的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX;
>
>2：查看当前锁定的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS;
>
>3：查看当前等锁的事务：SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;
>
>4：杀死进程：kill 线程ID

<span style="background:#f9eda6;">避免死锁的方法</span>：

1. 操作多张表时，尽量以相同的顺序来访问（避免形成等待环路) 

2. 避免长事务，小事务发送锁冲突的几率小。

3. 批量操作单张表数据的时候，先对数据进行排序（避免形成等待环路)

4. 尽量使用索引访问数据，避免没有where条件的操作（避免锁表）

5. 主键等值更新的时候，尽量先查询看数据库中有没有满足条件的数据，存在才更新（避免产生间隙锁）

6. 尽量使用主键更新数据，主键是唯一索引，在等值查询能查到数据的情况下只会产生行锁，不会产生间隙锁，这样产生死锁的概率就减少了。如果是范围查询，一样会产生间隙锁。

7. 在允许幻读和不可重复度的情况下，尽量使用RC的隔离级别，避免gap lock造成的死锁，因为产生死锁经常都跟间隙锁有关，间隙锁的存在本身也是在RR隔离级别来

   解决幻读的一种措施。

# Myisam和Innodb

MyISAM是MySQL5.5版之前的默认数据库引擎，虽然性能极佳，但不支持事务处理 （transaction），两者区别如下：

1. <span style="background:#f9eda6;">事务支持</span>

   MyISAM：强调的是性能，每次查询具有原子性，其执行数度比InnoDB类型更快，但是不提供事务支持。 

   InnoDB：支持事务，外部键等高级数据库功能。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务自动提交，宜合并事务，一同提交， 减小数据库多次提交导致的开销，提高性能。

2. <span style="background:#f9eda6;">锁差异</span>

   MyISAM：只支持表级锁，用户在操作 MyISAM 表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 

   InnoDB：支持事务和行级锁，是 InnoDB 的最大特色。行锁提高了多用户并发操作的性能。但是 InnoDB的行锁，只针对WHERE的条件是索引，非索引的WHERE会锁全表。

3. <span style="background:#f9eda6;">外键</span>

   MyISAM：不支持 InnoDB：支持

4. <span style="background:#f9eda6;">全文索引</span>

   MyISAM：支持(FULLTEXT类型的)全文索引 

   InnoDB：MySQL5.6 版本时 InnoDB 的版本升级到 1.2.x，此时支持全文索引

5. <span style="background:#f9eda6;">表主键</span>

   MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 

   InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。InnoDB的主键范围更大，最大是MyISAM的2倍。

6. <span style="background:#f9eda6;">表的具体行数</span>

   MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。 

   InnoDB：没有保存表的总行数(只能遍历)，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。

7. <span style="background:#f9eda6;">CURD操作</span>

   MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。 

   InnoDB：如果数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。 DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。

8. <span style="background:#f9eda6;">存储结构</span>

   MyISAM：有3个文件：Frm文件、MYD文件和MYI文件。Frm文件是表的定义文件，MYD文件是数据文件（所有的数据保存在这个文件中），MYI文件是索引文件。

   InnoDB：有2个文件：Frm文件和Ibd文件。Frm文件是表的定义文件，而Ibd文件是数据和索引存储文件（数据以主键进行聚集索引，把真正的数据保存在叶子节点中）。 InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。

   由上面可以看出，两者的区别就是<font color='Peach'>InnoDB把数据和索引都放在一个文件中，而MyISAM则是有两个文件分开存储。</font>通过索引文件和数据文件是否分离来分成聚集和非聚集索引。所以<font color='Peach'>InnoDB存储引擎是聚集索引，而MyISAM存储引擎是非聚集索引</font>。

9. <span style="background:#f9eda6;">存储空间</span>

   MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能 有空格，会被去掉)、动态表、压缩表。 

   InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。

10. <span style="background:#f9eda6;">可移植性、备份及恢复</span>

   MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针 对某个表进行操作。 

   InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。

11. <span style="background:#f9eda6;">查询效率</span>

    没有 where 的 count() 使用 MyISAM 要比 InnoDB 快得多。因为 MyISAM 内置了一个计数器，count() 时它直接从计数器中读，而 InnoDB 必须扫描全表。所以在 InnoDB 上执行 count() 时一般要伴随 where，且 where 中

    要包含主键以外的索引列。为什么这里特别强调“ 主键以外” ？因为InnoDB中 key primary index是和raw data 存放在一起的，而secondary index则是单独存放，然后有个指针指向primary 。所以只是count()的话

    使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回 raw data 时的作用较大。 MyISAM相对简单，所以在效率上要优于InnoDB，小型应用可以考虑使用MyISAM。


InnoDB自身很多良好的 特点，比如事务支持、存储过程、视图、行级锁定等等，在并发很多的情况下InnoDB的表现要比MyISAM强很多。

<span style="background:#f9eda6;">应用场景：</span>

1) MyISAM：适用于读多写少的场景，管理非事务表。它提供高速存储和检索，以及全文搜索能力。

2) InnoDB：适用于要求事务支持、数据一致性和并发控制的应用用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的 INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。

# 索引

## 分类

### 以数据结构区分

1. <span style="background:#f9eda6;">B-Tree 索引（平衡树索引）：</span>

   MySQL 默认的索引类型

- **主要特点：** MySQL中常用的索引类型之一，适用于等值查询、范围查询和排序查询。
- **构成方式：** B+Tree索引按照键值的顺序构建一个平衡树，每个节点包含多个键值对，并按照一定规则分配子节点。
- **适用场景：** 适合处理范围查询，例如`WHERE column = 'value'`或`WHERE column BETWEEN 'value1' AND 'value2'`。

2. <span style="background:#f9eda6;">哈希索引：</span>

   Memory引擎默认支持哈希索引。MySQL 8.0 版本开始支持 InnoDB 引擎的哈希索引，但是需要通过伪Hash索引来实现，叫自适应Hash索引。

   ~~~sql
   CREATE INDEX index_name ON table_name(column_name) USING HASH;
   ~~~

- **主要特点：** 基于哈希表的索引结构，将索引列的值计算哈希值并存储在哈希表中。
- **构成方式：** 哈希索引非常快速，但是只适用于等值查询（例如`WHERE column = 'value'`），不适合范围查询。
- **适用场景：** 适合用于全值匹配的场景，但是在范围查询等其他操作下效果不佳。

3. <span style="background:#f9eda6;">全文索引（Full-Text Index）：</span>

   仅适用于 `MyISAM` 和 `InnoDB` 存储引擎，并且只能应用于特定类型的文本列（如 `TEXT` 和 `VARCHAR`）。

   InnoDB 从1.2.x 开始支持全文索引。MySQL5.6 版本 InnoDB 的版本升级到 1.2.x。
   
   ~~~sql
   CREATE FULLTEXT INDEX index_name ON table_name(column_name);
   ~~~

- **主要特点：** 用于全文搜索的索引类型，允许快速检索大量文本数据。
- **构成方式：** 全文索引适用于对文本字段进行全文检索，例如在`TEXT`和`VARCHAR`字段上。
- **适用场景：** 适合需要对文本数据进行关键词搜索、模糊查询的场景。

4. <span style="background:#f9eda6;">空间索引（Spatial Index）：</span>

   ~~~sql
   CREATE SPATIAL INDEX index_name ON table_name(column_name);
   ~~~

- **主要特点：** 用于处理空间数据类型（如`GEOMETRY`，`POINT`，`LINESTRING`等）的索引。
- **构成方式：** 空间索引支持特殊的空间算法，可以快速查询包含或相交于特定区域的数据。
- **适用场景：** 适合处理地理信息系统（GIS）、位置数据等场景。

### 按物理存储分类

MySQL索引按叶子节点存储的是否为完整表数据分为：聚簇索引（主键索引）、非聚簇索引（二级索引）

1. <span style="background:#f9eda6;">聚簇索引</span>
   聚簇索引就是按照每张表的主键构造一颗 B+tree，同时叶子节点中存放的就是整张表的行记录数据，聚集索引的叶子节点被称为数据页。InnoDB表要求必须有聚簇索引，默认在主键字段上建立聚簇索引，在没有主键字段的情况下，表的第一个非空的唯一索引将被建立为聚簇索引，在即没有主键也没有非空的唯一索引的情况下，InnoDB将自动生成一个隐式的自增id列，并在此列上建立聚簇索引。

2. <span style="background:#f9eda6;">非聚集索引（也叫二级索引、辅助索引）</span>

   非聚集索引的结构和聚集索引基本相同（非叶子结点存储的都是索引指针），区别在于叶子节点存放的不是行数据而是数据主键。因此在使用非聚集索引进行查找时，需要先查找到主键值，然后再到聚集索引中进行查找。

<span style="background:#f9eda6;">两种索引的区别</span>：

	1. 每个索引上包含的字段内容不同：聚集索引包含所有真实的物理数据，非聚集索引只包含索引字段和主键字段。
	1. 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个。

**那非聚集索引这种查询方式算不算回表呢？**

> 回表查询简单来说就是通过非聚集索引查询数据时，得不到完整的数据内容，需要再次查询主键索引来获得数据内容。
>
> 所以如果使用非聚集索引后还需要使用其他字段的（包括在where条件中或者select子句中），则需要通过主键索引回表到聚集索引获取其他字段。
>
> 如果非聚集索引可以满足SQL语句的所有字段的，则被称为全覆盖索引，没有回表开销。
>
> 避免回表查询问题，常见的方式就是建立联合索引（组合索引），实现索引覆盖，从而避免回表查询。索引覆盖就是指索引的叶子节点已经包含了查询的数据，满足查询要求，没必要再回表进行查询。

### 按字段特性分类

MySQL索引按字段特性分类可分为：主键索引(PRIMARY KEY)、唯一索引(UNIQUE)、普通索引(INDEX)、全文索引(FULLTEXT)

1. <span style="background:#f9eda6;">主键索引(PRIMARY KEY)</span>
   建立在主键上的索引被称为主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。

2. <span style="background:#f9eda6;">唯一索引(UNIQUE)</span>
   建立在UNIQUE字段上的索引被称为唯一索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会发生重复冲突。

3. <span style="background:#f9eda6;">普通索引(INDEX)</span>
   建立在普通字段上的索引被称为普通索引。

4. <span style="background:#f9eda6;">全文索引(FULLTEXT)</span>
   MyISAM 存储引擎支持Full-text索引，用于查找文本中的关键词，而不是直接比较是否相等。Full-text 索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。

   InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持Full-text索引。

### 按索引字段个数分类

MySQL索引按字段个数分类可分为：**单列索引、联合索引（也叫复合索引、组合索引）**。

1. 单列索引
   建立在单个列上的索引被称为单列索引。

2. 联合索引（复合索引、组合索引）
   建立在多个列上的索引被称为联合索引，又叫复合索引、组合索引。在MySQL中使用联合索引时要遵循**最左前缀匹配原则**。所以我们需要注意如下几个方面：

   a. 实际业务场景中创建联合索引时，我们应该把识别度比较高的字段放在前面，提高索引的命中率，充分的利用索引。

   b. 创建联合索引后，该索引的任何最左前缀都可以用于查询。比如有一个联合索引(col1, col2, col3)，该索引的所有最左前缀为(col1)、(col1, col2)、(col1, col2, col3)，包含这些列的所有查询都会使用该索引进行查询。

   c. 虽然联合索引可以避免回表查询，提高查询速度，但同时也会降低表数据更新的速度。因为联合索引列更新时，MySQL不仅要保存数据，还要维护一下索引文件。所以应根据业务需求来创建。

## 数据结构

`MySQL `中索引的数据结构是 `B+Tree`，相比于 B-Tree 有这些优势：

> 简单概括就是`B-Tree `的非叶子结点会存放数据，而 `B+Tree` 的非叶子结点不存放数据，只存放关键字，这样同一个结点，`B+Tree `的结点能形成的叉数越多，那么存储一定数量的数据，`B+Tree` 的高度越小，那么插入、查找、删除的性能会越高。

> 通常树的插入、查找、删除的时间复杂度与树的高度成正相关，树越高，时间复杂度越高，性能越差。
>
> `MySQL `中存储数据是以页为单位的，在读数据时，也是以页为单位将数据从磁盘加载进内存当中，每从磁盘中读取一个页，就会发生一次磁盘 `IO`。也就是说，在查找数据时，每遍历一个结点，就意味着读取一个数据页，也就是发生一次` IO`。
>
> 对于` B-Tree `而言，如果要查找的数据在树的越底层，就意味着要发生的` IO `次数越多，树越高，查找时可能发生的磁盘 `IO` 次数越多，磁盘 `IO `越多，意味着程序的耗时会越长，性能越差。
>
> 而对于 `B+Tree `而言，它的树高相对 `B-Tree` 而言会低很多，而且所有的数据都是存放在叶子结点当中的，所以查询数据时，一定是遍历到叶子结点层，时间复杂度相对稳定，而且发生的磁盘 IO 次数较少，所以整体来讲，`B+Tree` 性能更优，因此 `MySQL` 的索引数据结构选择的是 `B+Tree`。
>
> 通常一颗 `B+Tree `的高度为 `3` 或者` 4`，这样一次索引的查找通常最多只需要发生 `3-4 `次磁盘 `IO`，而 `MySQL `中由于` buffer pool `缓冲池的机制，第一层的结点所对应的数据页通常存在于缓存中，第二层的数据页也有很大可能也存于缓存中（`MySQL` 的预读机制、`LRU `缓存淘汰机制等都有可能导致），因此在遍历时，`B+Tree` 的第一层、第二层极有可能不会发生磁盘` IO`，而是基于内存查找，所以一次数据查找，可能就只会发生 `1-2 `次磁盘 `IO`，这个查询效率得到了大大提高。
>
> 另外，`B+Tree` 的叶子结点中，存放了相邻叶子结点的指针，因此在进行范围查找时，不需要多次遍历整棵树，只需要找到一个叶子结点后，通过指针，就能找到其他的叶子结点，这是十分高效的一种做法。

### 为何选择B+TREE

1. B+树能显著减少IO次数，提高查询效率

   > B+ 树非叶子节点上不存储数据，仅存储键值，之所以这么做是因为在数据库中页的大小是固定的，InnoDB 中页的默认大小是 16KB。如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数又会再次减少，数据查询的效率也会更快。
   > 另外，B+ 树的阶数是等于键值的数量的，如果我们的 B+ 树一个节点可以存储 1000 个键值，那么 3 层 B+ 树可以存储 1000×1000×1000=10 亿个数据。
   > 一般根节点是常驻内存的，所以一般我们查找 10 亿数据，只需要 2 次磁盘 IO。

2. B+树的查询效率更加稳定，因为数据放在叶子节点

3. B+树能提高范围查询的效率，因为叶子节点指向下一个叶子节点

<span style="background:#f9eda6;">为何不使用 B 树</span>

1. B树只适合随机检索，而B+树同时支持随机检索和顺序检索。

2. B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。

   > 一般来说，索引本身也很大，不可能全部存储在内存中，往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的非叶子结点只存储关键字不存储数据，内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；

3. B+树的查询效率更加稳定。

   > B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短， 只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 

4. B树元素遍历效率低下。

   > B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 

5. 增删效率更高。

   > 在 B+ 树中，所有数据都存储在叶子节点中，而非叶子节点仅存储索引。当插入或删除数据时，只需调整索引，数据不会发生移动，这降低了插入和删除操作的复杂度。

<span style="background:#f9eda6;">为何不选择数组、哈希表、二叉搜索树、红黑树等数据结构作为索引？</span>

> MySQL 作为存储数据的组件，主要操作就是数据的**增删改查**，其中<span style="background:#f9eda6;">查询</span>操作又是重中之重。我们经常所说的数据库优化，大部分优化的就是查询相关的操作。因此一个数据库选择何种数据结构作为索引，主要考虑因素就是这种数据结构对增删改查操作的效率如何，尤其是查询操作（通常查询操作包括等值查询、范围查询等）。

1. **数组**：不适合插入操作

   > <font color='each'>定义</font>：数组是内存中一块连续的内存空间，定义一个数组对象，这个对象的指针指向了这块内存的起始地址，如果知道数组元素的下标，就可以计算出该下标所对应的元素的内存地址了，因此可以在` O(1)`的时间复杂度内获取到元素，非常快速。
   >
   > <font color='each'>优点</font>：对于一个<font color='Magenta'>有序数组</font>，它的<font color='Magenta'>查询、删除、更新操作</font>的效率非常高。查找过程可以使用二分查找法，时间复杂度为 `O(logn)`。因为是有序数组，因此对范围查找也十分友好，只需要找到起始元素即可。如果在知道元素下标的情况下，更新操作也非常快，对于删除操作，如果我们不考虑空洞的话（如果直接将对应下标处的元素置为 null，这样这块连续内存块中相当于有个空洞），删除操作也很快。
   >
   > <font color='each'>缺点</font>：<font color='Magenta'>数组插入效率很低</font>。要往数组中间插入一个数据，需要将数组中要插入的目标位置后的所有元素先往后挪动一个位置，然后才能插入新的数据，也就是涉及到了数组的复制操作，要插入的数据越靠前，需要复制的数据就越多，不仅需要额外开辟内存，复制数据消耗的时间也很长。而在平时的开发中，生产环境的一张表的大小动辄就 1GB 以上了，如果要往中间插入一条数据时，那得复制多少数据

   因此，从插入数据这一角度来看，数组不太适合作为 MySQL 索引的数据结构。

2. **哈希表**：虽然可以快速定位，但是没有顺序，不适合范围查询，IO复杂度高。

   > <font color='each'>定义</font>：哈希表是一种 `key-value` 形式的数据结构，底层采用<font color='Magenta'>数组+链表</font>结构来实现，将 `key `通过一个哈希函数计算出一个数字，然后以该数字作为数组的下标，然后将 `value `存放到对应下标的数组中。对于不同的 `key`，在经过哈希函数计算后，可能出现相同的值(哈希冲突)，这时候就意味着同一个数组下标处要存放两个元素了，所以这个时候将数组中的元素变为一个链表，通过链表将这两个元素串联起来。
   >
   > <font color='each'>优点</font>：哈希表对于<font color='Magenta'>删除、查找、更新、插入</font>操作，都是先根据 `key `计算出一个值，就能定位到数据的目标位置了，时间复杂度都是` O(1)`，速度特别快。
   >
   > <font color='each'>缺点</font>：范围查找性能不行，例如 between...and、>=、<=等。因为哈希表的所有 `key `都会经过哈希函数计算，然后再存放数据，本来可能有序的 `key`，但经过哈希函数计算出来的值就不是有序的了，<font color='Tasma'>如果要在哈希表中进行范围查找，就只能对整个哈希表进行遍历</font>，只有符合条件范围的数据，才取出来。当我们数据库中的数据越来越多，达到几百万甚至几千万条的时候，这个时候，对整个表的遍历是非常耗时的。
   >
   > <font color='each'>适用场景</font>：哈希表适用于等值查询的场景，最经典的场景就是 `NOSQL` 数据库，例如最常用的 `Redis`，`Redis `中全是 `key-value `

3. **二叉树**：树的高度不均匀，不能自平衡，查找效率跟树的高度有关，并且IO代价高。

   > <font color='each'>定义</font>：每个节点最多有两个子结点的树称之为二叉树，比较特殊且常用的二叉树有<font color='Magenta'>二叉搜索树、`AVL` 树（平衡树）、红黑树等</font>
   >
   > <font color='each'>特点</font>：对于二叉搜索树而言，它的<font color='Magenta'>查找操作的时间复杂度就是树的高度</font>，最理想的情况下，也就是满二叉树的情况下，查找的时间复杂度为 `O(logn)`。
   >
   > <font color='each'>缺点</font>：当不停地动态地往树中插入数据、删除数据时，在极端情况下，二叉搜索树可能退化成链表，它的查找时间复杂度就变成了 `O(n)`，性能不够稳定。
   >
   > 

4. **红黑树**：树的高度随着数据量增加而增加，IO代价高。

   > <font color='each'>平衡树</font>：平衡树是在二叉查找树的基础上，增加了一条限制，左右两个子树的高度差不能超过 `1`，左右两边相对平衡。
   >
   > <font color='each'>定义</font>：在平衡二叉树的基础上又出现了红黑树，整体上来说，红黑树是一种近似平衡（不完全平衡），结点非黑即红的树，它的树高最高不会超过 `2logn`，因此查找的时间复杂度为 `O(logn)`，无论是增删改查，它的性能都十分稳定。工程上，很多地方都使用的是红黑树这种数据结构，例如 `Java` 中的` HashMap、TreeMap `等。
   >
   > <font color='each'>优点</font>：查找、删除、插入、更新的复杂度均为` O(logn)`。它的中序遍历，数据是有序的，因此也适合范围查找。
   >
   > <font color='each'>缺点</font>：为了维护平衡，它的旋转操作过于复杂；平衡树在数据动态的删除、插入过程中，为了维护平衡，避免树退化成链表，需要在<font color='Magenta'>删除或者插入数据后进行额外的旋转操作，会损耗一定的性能</font>。
   >
   > <font color='each'>总结</font>：不论是二叉搜索树、 `AVL `树，还是红黑树，都是二叉树的一种，<font color='Magenta'>每个结点最多只有两个子结点，如果存储大量数据的话，树的高度会非常高</font>。而 `MySQL `存储的数据最终是要落地到磁盘的，`MySQL `应用程序读取数据时，需要将数据从磁盘先加载到内存后才能继续操作，所以这中间会发生磁盘 `IO`，而如果树太高，每遍历一层结点时，就需要从磁盘读取一次数据，也就是发生一次 `IO`，如果数据在树高为 `20 `的地方，那查找一次数据就得发生 `20 `次 `IO`，这对应用程序简直就是灾难性的，耗时太长了。因此二叉树在 `MySQL `这种需要存储大量数据的场景下，是不适合当做索引的数据结构的，因为<font color='Magenta'>树太高，操作数据时会发生多次磁盘 `IO`，性能太差。</font>

### InnoDB一棵B+树可以存放多少行数据?

三层 B+树可存储约2千万行。

> InnoDB存储引擎最小储存单元是页，一页大小就是16k。因为B+树叶子节点存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据;
>
> 假设B+树的高度为2的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为 = 根结点指针数 * 单个叶子节点记录行数。
>
> - 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数 =16k/1k =16.
> - 非叶子节点内存放多少指针呢？我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，所以就是8+6=14字节，16k/14B =16*1024B/14B = 1170
>
> 因此，一棵高度为2的B+树，能存放1170 * 16=18720条这样的数据记录。同理一棵高度为3的B+树，能存放1170 *1170 *16 =21902400，也就是说，可以存放两千万左右的记录。B+树高度一般为1-3层，已经满足千万级别的数据存储。

## <span id="jump">索引创建与优化</span>

1. <span style="background:#f9eda6;">索引创建原则：</span>
   - **识别度高的列：** 对经常用于查询且识别度高的列创建索引，避免对低选择性的列创建索引，以减少索引大小和提高查询效率。
   - **索引字段越小越好：** 数据库的数据存储以页为单位，一页存储的数据越多一次IO操作获取的数据越大 效率越高。
   - **组合索引：** 如果多个列经常一起用于查询，可以考虑创建组合索引，减少查询时的索引选择。
   - 频繁进行数据操作的表，不要建立太多的索引
   - 在经常搜索的列上加索引
   - 频繁在where子句中出现的字段建立索引
   - 经常进行GROUP BY、ORDER BY、 distinct 的字段上建立索引
   - 经常与其他表进行连接的表，在连接字段上应该建立索引

2. <span style="background:#f9eda6;">适当优化索引结构：</span>
   - **适度分解索引：** 对于写入频繁的表，可能需要对索引进行适当的分解，避免索引过大造成的额外开销。例如复合索引的各列并不都在查询中被使用，某些查询可能只需要其中的一部分列。
   - **尽量避免过度索引：**  索引通常采用B+树来实现，每个索引都需要维护一个独立的树结构占用一定的存储空间。当索引过多时会增加数据库的内存和磁盘开销。且每次对表进行插入、更新、删除等操作时，都需要更新索引结构，会降低写入性能。过多的索引可能会导致维护开销过大和额外的存储空间消耗。

3. <span style="background:#f9eda6;">注意查询优化：</span>

   - **使用覆盖索引：** 尽量设计覆盖索引，即索引包含了查询所需的全部信息，避免回表操作，提高查询效率。

   - **like语句的前导模糊查询不能使用索引**

     ~~~sql
     select * from doc where title like '%XX'；   --不能使用索引
     ~~~

   - **union、in、or 都能够命中索引，建议使用 in**

     a. `union`能够命中索引，耗费的 CPU 最少。

     ```sql
     select * from doc where status=1
     union all
     select * from doc where status=2;
     ```

     b. `in`能够命中索引，查询优化耗费的 CPU 比 `union all` 多，但可以忽略不计，一般情况下建议使用 `in`。

     ```sql
     select * from doc where status in (1, 2);
     ```

     c. `or` 新版的 MySQL 能够命中索引，查询优化耗费的 CPU 比 `in`多，不建议频繁用`or`。

     ```sql
     select * from doc where status = 1 or status = 2
     ```

     **补充**：有些地方说在`where`条件中使用`or`，索引会失效，造成全表扫描，这是个误区：

     - ①要求`where`子句使用的所有字段，都必须建立索引；
     - ②如果数据量太少，MySQL 制定执行计划时发现全表扫描比索引查找更快，所以会不使用索引；
     - ③确保mysql版本`5.0`以上，且查询优化器开启了`index_merge_union=on`, 也就是变量`optimizer_switch`里存在`index_merge_union`且为`on`。

   - **联合索引最左前缀原则**

     a. 建立联合索引的时候，区分度最高的字段在最左边

     b. 存在非等号和等号混合判断条件时，在建立索引时，把等号条件的列前置。如 `where a>? and b=?`，那么即使`a` 的区分度更高，也必须把 `b` 放在索引的最前列。

   - **不能使用索引中范围条件右边的列（范围列可以用到索引），范围列之后列的索引全失效**

     - 范围条件有：`<、<=、>、>=、between`等。

       ~~~sql
       select * from employees.titles where emp_no < 10010 and title='Senior Engineer'; -- emp_no < 10010后的条件无法使用索引
       ~~~

   - **索引列上面做任何操作（计算、函数），都会导致索引失效而转向全表扫描**

     ~~~sql
     select * from order where date < = CURDATE()；
     -- 优化为具体时间
     select * from order where date < = '2018-01-2412:00:00';
     ~~~

   - **强制类型转换会导致全表扫描**

   - **更新十分频繁、数据区分度不高的列不宜建立索引**

     - 更新会变更 B+ 树，更新频繁的字段建立索引会大大降低数据库性能。
     - 一般区分度在80%以上的时候就可以建立索引，区分度可以使用 `count(distinct(列名))/count(*)` 来计算。

   - **利用覆盖索引来进行查询操作，避免回表，减少select * 的使用**

     - 覆盖索引：查询的列和所建立的索引的列个数相同，字段相同。

   - **字段描述合理使用 not null**

     - 复合索引中只要有一列含有`NULL`值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时，尽量使用`not null` 约束以及默认值。

   - **IS NULL 和 IS NOT NULL 查询的情况**

     - **单列索引：** 对于单列索引，数据库通常可以利用索引来执行 `IS NULL` 和 `IS NOT NULL` 的查询，因为索引已经按照列的值排序，NULL 值和非 NULL 值可以被索引区分开来。
     - **复合索引：** 在涉及到复合索引时，如果查询条件只涉及复合索引中的一部分列或不是索引的最左侧前缀部分，可能会导致索引失效。
     - 当某个列的数据分布中 NULL 值占据了很大一部分时，使用 `IS NULL` 或 `IS NOT NULL` 查询可能不会使用索引，因为优化器可能认为全表扫描更为高效。

## 索引失效

1. 如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
2. 联合索引不满足最左匹配原则。
3. 模糊查询时（like语句），模糊匹配的占位符位于条件的首部
4. 索引列的数据类型存在隐形转换，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引
5. 索引列上有数学运算、函数时，索引失效
6. 如果 MySQL 估计使用全表扫描要比使用索引快，则不使用索引（比如数据量少的表）
7. 避免在 where 子句中使用 != 或 <> 操作符

## 索引下推

<font color='RedOrange'>简介：</font>ICP（`Index Condition Pushdown`）是在MySQL 5.6版本上推出的查询优化策略，把本来由`Server`层做的索引条件检查下推给存储引擎层来做，以降低回表和访问存储引擎的次数，提高查询效率。

<font color='RedOrange'>原理：</font>

> 没有使用ICP的情况下，MySQL查询过程如下：
>
> - 存储引擎读取索引记录；
> - 根据索引中的主键值，定位并读取完整的行记录（回表）；
> - 存储引擎把记录交给`Server`层去检测该记录是否满足`WHERE`条件。
>
> 使用ICP的情况下，查询过程如下：
>
> - 读取索引记录（不是完整的行记录）；
> - 判断`WHERE`条件部分能否用索引中的列来做检查，条件不满足，则处理下一行索引记录；
> - 条件满足，使用索引中的主键去定位并读取完整的行记录（回表）；
> - 存储引擎把记录交给`Server`层，`Server`层检测该记录是否满足`WHERE`条件的其余部分。

<font color='RedOrange'>使用条件</font>：

- 只能用于`range`、 `ref`、 `eq_ref`、`ref_or_null`访问方法；
- 只能用于`InnoDB`和 `MyISAM`存储引擎及其分区表；
- 对`InnoDB`存储引擎来说，索引下推只适用于二级索引（即非主键索引）对主键索引无效;

> 索引下推的目的是为了减少回表次数，也就是要减少IO操作。对于`InnoDB`的**聚簇索引**来说，完整的行记录已经加载到缓存区了，索引下推也就没什么意义了。

- 引用了子查询的条件不能下推；
- 引用了存储函数的条件不能下推，因为存储引擎无法调用存储函数。

# `SQL`慢查询（优化）

## 查看执行计划

其中最重要的是`type`（`join`类型）、`rows`、`filtered`、`key`列（最终使用的索引）和`extra`列（额外的信息）。

- **type**

  > - ALL：全表扫描
  > - index：索引物理文件全扫描，速度非常慢
  > - range：索引范围扫描，常用语<,<=,>=,between,in等操作
  > - ref：使用非唯一索引扫描或唯一索引前缀扫描，返回单条记录，常出现在关联查询中
  > - eq_ref：类似ref，区别在于使用的是唯一索引，使用主键的关联查询
  > - const/system：单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。，系统会把匹配行中的其他列作为常数处理，如主键或唯一索引查询
  > - null：MySQL不访问任何表或索引，直接返回结果
  >
  > 虽然上至下，效率越来越高，但是根据cost模型，假设有两个索引`idx1(a, b, c)`,`idx2(a, c)`，SQL为`select * from t where a = 1 and b in (1, 2) order by c;`如果走idx1，那么是type为range，如果走idx2，那么type是ref；当需要扫描的行数，使用idx2大约是idx1的5倍以上时，会用idx1，否则会用idx2
  >
  > 
  >
  > SQL性能优化的目标：**至少要达到 range 级别**，要求是ref级别，如果可以是 const 最好。 

- **rows**

  > 如果查询优化器决定使用**全表扫描**的方式对某个表执行查询时，执行计划的`rows`列就代表预计需要扫描的**行数**，如果使用**索引**来执行查询时，执行计划的`rows`列就代表预计扫描的**索引记录行数**

- **filtered**

  >如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。
  >
  >如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。

- **extra**

  <font color='Chestnut Red'>**Extra中出现`Using filesort`、`Using temporary`代表MySQL根本不能使用索引，效率会受到严重影响，应当尽可能的去优化。**</font>

  > - **Using index**：使用了覆盖索引
  >
  >   使用了覆盖索引，直接从索引中就能获取到结果。查询列表和查询条件只包含了某个索引中的列，直接通过索引就能获取到结果，不需要进行回表。如果同时出现 using where，意味着无法直接通过索引查找来查询到符合条件的数据。
  >
  > - **Using index condition**：
  >
  >   MySQL5.6之后新增的ICP，using index condtion就是使用了ICP（索引下推），在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据。
  >
  > - **Using where**：使用了where子句来过滤结果集
  >
  >   在查找使用索引的情况下，索引并不能覆盖到需要查询的所有列，需要回表去查询所需的数据。(where条件中除了索引包含的列外，还有索引未包含的列)
  >
  >   当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息
  >
  >   当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。
  >
  > - **Using filesort**：使用文件排序，当查询语句包含ORDER BY时如果无法使用索引来完成排序时出现，非常消耗性能，尽量优化。
  >
  >   MySQL需要额外的一次传递，以找出如何按排序顺序检索行。通过根据联接类型浏览所有行并为所有匹配WHERE子句的行保存排序关键字和行的指针来完成排序。然后关键字被排序，并按排序顺序检索行。
  >
  > - **Using temporary**：
  >
  >   使用临时表来保存中间结果，用于完成排序、去重等操作。比如我们在执行许多包含 DISTINCT 、 GROUP BY 、ORDER BY、 UNION 等子句的查询过程中，如果不能有效利用索引来完成查询， 就需要建立内部的临时表来执行查询。性能特别差，需要重点优化

## 查询`SQL`执行细节

`Show Profile`是`MySQL`提供的可以用来分析当前会话中`sql`语句执行的资源消耗情况的工具，可用于`sql`调优的测量

```sql
show profile cpu,block io for query Query_ID;
```

需要注意以下几种`status`:

> ①converting HEAP to MyISAM：查询结果太大，内存不够，数据往磁盘上搬了。
>
> ②Creating tmp table：创建临时表。先拷贝数据到临时表，用完后再删除临时表。
>
> ③Copying to tmp table on disk：把内存中临时表复制到磁盘上，危险！！！
>
> ④locked。
> 如果在show profile诊断结果中出现了以上4条结果中的任何一条，则`sql`语句需要优化。

## 慢SQL的原因

### 没有命中索引及索引失效

```markdown
首先分析一下为什么没有命中索引会导致慢查询。
	索引本质就是通过列数据构建成的B+树结构。构建索引的目的是将无序的数据有序化，尽量避免全文扫描，提高检索效率。
	没有命中索引意味着在查询的过程中没有方向，全文检索，效率极差。
	而判断出sql使用了索引，但是仍然慢则需要进行索引优化了。
```

- 造成索引失效的可能原因

```markdown
1、如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
2、联合索引不满足最左匹配原则。
3、模糊查询时（like语句），模糊匹配的占位符位于条件的首部
4、存在索引列的数据类型隐形转换，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
5、索引列上有数学运算，用不上索引
6、索引列使用函数，用不上索引
7、如果mysql估计使用全表扫描要比使用索引快,则不使用索引（比如数据量少的表）
8、避免在 where 子句中使用 != 或 <> 操作符
```

总结下来，索引会失效大部分原因都是**由于破坏了索引本身的连续性所导致的**。对于单索引而言，采用like以%开头，无法找到索引字段前缀元素，因此无法使用到索引，而对于联合索引而言，or / where 中带有运算&函数等，均是破坏了联合索引字段中的连续性，导致后续索引找不到。

- 索引优化场景

  - [**索引创建与优化**](#jump)
  - 对于单索引且索引字段内容很长时的优化

  > **1、构建前缀索引（使用短索引）**：
  > 默认索引是选择字符列的全部，那可以只选择索引开始的部分字符，例如，如果有一个CHAR(255)的列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。这样就减少了索引的空间，从而提高索引效率。**如何选择前缀长度**， 这里涉及到索引选择性的问题。在选择前缀长度时要考虑选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）,最后还要考虑数据分布。一般，选择索引长度的方式，先计算出完整列的选择性，再选择出合适的前缀长度。
  >
  > **2、加入伪hash字段**
  > 新建一列用于存储该字符列的hash值（哈希函数不要使用SHA1(),MD5(),因为会产生很长的字符串，浪费空间，比较也慢，最好是返回整数的hash函数），在该列建立索引，查询时必须在where子句中包含常量值，以避免hash冲突。

  - 对于联合索引，需要特别注意索引顺序

  > **最左前缀原则**：索引的最左前缀和B+Tree中的“最左前缀原理”有关，举例来说就是如果设置了组合索引<col1,col2,col3>那么以下3中情况可以使用索引：col1，<col1,col2>，<col1,col2,col3>，其它的列，比如<col2,col3>，<col1,col3>，col2，col3等等都是不能使用索引的。
  > 根据最左前缀原则，我们一般把排序分组频率最高的列放在最左边，以此类推。

  - 带索引的模糊查询优化

  > 使用LIKE进行模糊查询的时候，’%aaa%'不会使用索引，也就是索引会失效。如果是这种情况，只能使用全文索引来进行优化。
  > 解决方案：
  > **覆盖索引**，所谓覆盖索引，简单理解，就是每次select出来的字段仅从索引列就可以找到，不需要回表查询。比如select (主键ID)。
  > **改造写法（翻转）**
  > 将前模糊进行反转，转化成走后模糊的语句（注意逻辑变化）。

### 单表数据量过大

> - 分页查询(在索引上完成排序分页操作、借助主键进行关联)
> - 单表数据过大，进行分库分表
> - 考虑使用非关系型数据库提高查询效率
> - 全文索引场景较多，考虑使用 ElasticSearch、solr

### Limit深分页

~~~sql
select id,name,balance from account where create_time> '2020-09-19' limit 100000,10;
# limit 深分页，导致 SQL 变慢原因有两个：
# 	limit 语句会先扫描 offset+n 行，然后再丢弃掉前 offset 行，返回后 n 行数据。也就是说 limit 100000,10，就会扫描 100010 行，而 limit 0,10，只扫描 10 行。
# 	limit 100000,10 扫描更多的行数，也意味着回表更多的次数。
~~~

可以通过减少回表次数来优化。一般有**标签记录法和延迟关联法**。

**标记记录法**

~~~sql
# 标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描
select  id,name,balance FROM account where id > 100000 limit 10;
~~~

**延迟关联法**

~~~sql
# 把条件转移到主键索引树，然后减少回表
select  acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN (SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) AS acct2 on acct1.id= acct2.id;
~~~

### 锁竞争

如果查询涉及到了大量的数据行，并且这些数据行被其他事务持有锁定，可能导致查询阻塞等待锁的释放，从而造成查询性能下降。可以用 `show processlist` 命令，看看当前语句处于什么状态

### InnoDB 刷脏页

InnoDB 引擎采用 Write Ahead Log(WAL)策略，即事务提交时，先写日志(redo log)，再写磁盘。 **为了提高 IO 效率,在写日志的时候会先写 buffer，然后集中 flush buffer pool 到磁盘。** 这个过程称之为 **刷脏页** 。如果脏页刷新频率过高或者脏页刷新量过大，会导致磁盘IO负载增加，从而影响其他查询的性能。当有长事务/异常未提交的情况就会因为其他查询需要构建快照导致 undo 不能被及时回收。查询遍历的 undo 越多 sql 执行的越慢。

> **应对策略**：
>
> 1. **增加缓冲池大小**：适当增加 InnoDB 的缓冲池大小，可以减少脏页刷新的频率，提高查询性能。
> 2. **优化查询语句**：通过优化查询语句、创建合适的索引、避免全表扫描等方式，减少对数据库的访问，从而减少脏页刷新的触发。（当内存不够用时，一个查询涉及多个脏页的淘汰，会导致查询的响应时间明显变长）


## 加了索引依旧慢

首先查看是否存在频繁回表的情况，减少频繁回表的开销

<font color='Magenta'>索引优化的过程，实质上是减少扫描行数的过程</font>

慢查询归纳起来大概有这么几种情况：

> - 全表扫描
> - 全索引扫描
> - 索引过滤性不好
> - 频繁回表的开销

首先SQL判断一个语句是不是慢查询语句，用的是语句的执行时间。他把语句执行时间跟long_query_time这个系统参数作比较，如果语句执行时间比它还大，就会把这个语句记录到慢查询日志里面。<font color='RedOrange'>是否使用索引和是否进入慢查询之间并没有必然的联系。使用索引只是表示了一个SQL语句的执行过程，而是否进入到慢查询是由它的执行时间决定的，而这个执行时间，可能会受各种外部因素的影响。换句话来说，使用了索引你的语句可能依然会很慢。</font>

`InnoDB`是索引组织表，所有的数据都是存储在索引树上面的。`InnoDB`里面只有一种情况叫做没有使用索引，那就是从主键索引的最左边的叶节点开始，向右扫描整个索引树。

- 可以用全表扫描来表示一个查询遍历了整个主键索引树；
- 也可以用全索引扫描，来说明他扫描了整个普通索引树；

我们平时说的使用了索引。他表示的意思是，我们使用了索引的快速搜索功能，并且有效的减少了扫描行数。<font color='RedOrange'>对于一个大表，在设计表结构的时候，不止要有索引，索引的过滤性还要足够好。</font>

## 执行流程

MySQL一共六个组件

- <font color='RedOrange'>连接器</font>：连接mysql服务器，进行权限验证
- <font color='RedOrange'>缓存</font>：保存上次查询的结果，提高性能
- <font color='RedOrange'>分析器</font>：词法与语法分析
- <font color='RedOrange'>优化器</font>：对你的查询语句做出适当的优化
- <font color='RedOrange'>执行器</font>：操作存储引擎，读写数据
- <font color='RedOrange'>存储引擎</font>：存储数据

<img src="https://gitee.com/qc_faith/picture/raw/master/image/20220502230440.png" alt="image-20220502230341754" style="zoom: 50%;" />

MySQL执行一个查询操作的完整流程为：<font color='Peach'>客户端通过连接器建立连接，这个操作进行权限验证，通过之后会先前往缓存，根据 SQL 作为 key 去查询，查到直接返回，否则前往分析器，经过分析器对 SQL 语句的分析、解析，得到一个MySQL可以理解的语法，随后进入优化器，MySQL会根据查询的条件进行适当的优化，之后在经过执行器，这就真正的开始前往存储引擎查询数据，最后将查询到的数据返回给客户端，顺便写入缓存（不一定）。</font>

MySQL基本分为两大组件，一个是Server层、一个是存储引擎，存储引擎是可以随场景变化的

### 连接器

位于Server层中，主要用于连接和权限验证，校验通过之后MySQL会返回当前登录用户所拥有的权限，用于后续操作。

若客户端一段时间没有使用则会被置为空闲（Sleep）状态，如果客户端长时间没有操作，服务器就会自动将连接断开，**默认时长 8 小时**，可以通过 **wait_timeout** 参数设置。

超过时长，连接被断开之后，客户端发起请求，那么该客户端将会收到一个：`Lost connection to MySQL server during query`，这个时候就只能通过重新建立连接进行操作。

连接分为两种类型，一个为长连接，一个为短连接，建立连接之后，客户端发送持续请求，如果一直在同一个连接中，那么这个就是长连接，如果每个请求一个连接，则是短连接，我们知道连接会有用户信息的校验，权限的验证，比较麻烦，所以推荐使用长连接进行操作，但是长连接过多会导致MySQL占用的内存过多，导致内存紧张，极端情况可能导致内存泄漏（OOM），所以需要定时清除长连接。通过执行 `mysql_reset_connection` 来重新初始化连接资源，不过要求MySQL版本在5.7或之上。

### 缓存

连接完成之后，下一步就会进入缓存，MySQL会在缓存中检测之前是否执行过这条语句，如果被执行过，那么查询的结果将会以`key-value`的形式存储在缓存中，这个时候下一次的查询直接命中缓存，直接返回相对应的数据，如果缓存中不存在当前`key`（SQL语句），就会进入下一个阶段 -- 分析器。

MySQL判断缓存是否命中的方式很简单，MySQL将缓存存放在一个引用表中，通过`hash`值方式应用，`hash`值包括：查询的SQL、查询的数据库、客户端协议版本等等，MySQL在判断是否命中缓存的时候不会提前解析SQL的语法，而是直接使用SQL语句和客户端的基本信息（协议）等等，进行`hash`算法，这里需要特别注意：**在编写SQL的时候，需要与上一次执行的SQL保持完全一致，空格、注释、编码或者有其他的任何不同的地方都会导致hash出来的结果不同，从而无法命中缓存**，所以在操作时需要保持一个统一的编码规范。

除了这个还有很多情况也会导致查询的数据无法缓存，比如聚合函数、自定义函数、存储过、用户变量、临时表、MySQL库中的系统表、权限表。

MySQL的缓存虽然能提升查询的性能，但是也会在其他方面造成额外的消耗，具体如下：

> 查询之前必须先检查是否命中缓存，对于缓存中没有的SQL多了一次缓存的查询。第一次查询或者表中的数据被修改时，当前查询需要将结果写入到缓存中，带来了额外的系统消耗。MySQL在写操作时会将关于当前相关缓存的数据全部清空，如果缓存的数据过大，或者缓存的SQL语句过多，可能会导致很大的系统消耗。

所以，缓存的好处可以提升查询的效率，弊端可能给系统带来额外的系统消耗，尤其是在`InnoDB`中的事务中，所以在使用的时候需要慎重，不可为了查询效率二盲目的使用缓存，使用不当，可能适得其反。

那MySQL开启缓存只需要将参数 **query_cache_type 设置成 DEMAND** 即可，这样会导致整个MySQL都是使用缓存，这是不被推荐的，所以还有一种方式，那就是按需指定，就是在你需要缓存的SQL语句加上 **SQL_CACHE** 指定使用缓存即可，代码如下：

```SQL
select SQL_CACHE * from sys_user where id = 1;
```

在这里有两点需要特别注意，一：MySQL在8.0版本直接将缓存模块删除，也就是说，MySQL8.0所有的查询都不会走向缓存了，而是直接前往磁盘；二：查询缓存的返回直接也会校验权限信息的，如果没有权限，就算使用了缓存，也无法查询。

### 分析(解析)器

MySQL在缓存中没有命中之后将会进入流程的下一步，但这里并不会直接进入解析器，而是需要先将查询的SQL转换成一个执行计划，在经过这个执行计划和存储引擎进行交互，这里就包括了**：解析器、预处理、优化器、执行器**。

生成完执行计划之后，MySQL会对SQL的关键字进行解析，生成一棵对应的 “**解析树**”，在这个解析过程中，MySQL解析器会使用语法规则对SQL进行解析和校验，第一步做的是词法分析，MySQL执行的并不是你写的SQL语句，而是将你写的SQL语句解析成MySQL可以执行的语句。

生成“解析树“之前还需要校验你的SQL语句写的是否有问题，是否满足MySQL的语法，如果你输入的SQL语句存在问题，这个时候程序将会抛出异常，结束查询。

先解析SQL语法，然后再预处理的时候再校验表名、字段名等是否合法。下一步预处理还会验证权限，这里的验证一般情况下都比较快，除非权限配置相当复杂。

### 优化器

分析器完成之后，语法树已经是合法的了，这个时候优化器就登场了，优化器将这条语法树**转化成执行计划**。MySQL官方也是很为我们开发人员着想，设置了这个优化器，在开发过程中，我们想要查询一条SQL语句，执行的方式有很多，比如是否走索引、走哪条索引、关联查询哪张表做主表等等，这些都是可变的，而优化器的作用就是根据程序员写的SQL语句找到一条它认为最好的执行计划。

并非经过优化器优化的SQL就是最优的执行计划，会导致优化器生成的执行计划效果更差的有以下七点：

1. 统计信息不完整或者不准确，比如 `InnoDB `的`MVCC`多版本并发控制会导致表数据行的统计不准确。
2. 执行计划中的成本并不等同于实际执行的成本，这个时候即使统计的信息很准确，优化器给出的执行计划也有可能不是最优的。举个例子，有些时候某个执行计划虽然需要读取更多的页面或者数据，但是它的实际成本可能会很小，为什么呢？原因很简单，如果读取的页面都是有序的或者这些页面(数据)已经被加载到内存中了，这个时候的访问成本比执行计划估计的成本小得多，MySQL并不知道哪些数据存在内存，哪些数据存在磁盘中，所以IO的次数也是未知数。
3. MySQL的最优可能和你想得不一样，你可能希望执行时间越短越好，但是MySQL只是基于成本模型选择最优的执行计划，而有些时候这并不是最快的执行方式。所以这里我们看到的根据执行计划成本来选择执行计划并不是完美的模型。
4. MySQL从不考虑其他并发执行的查询，这可能影响到当前查询的速度。
5. MySQL并不是完全基于成本优化，有时候也会给予一些固定的规则，例如存在全文索引的`match()`子句，则在存在全文索引的时候就是用全文索引，即使有时候使用别的索引和`where`条件可以远比全文索引的方式要快很多，但是MySQL也会选择使用全文索引。
6. MySQL不会考虑不受其控制的操作成本，例如存储过程或者用户自定义函数的成本。
7. 优化器有时候无法估算所有可能的执行计划，所以它可能错过实际上最优的执行计划。

MySQL的优化器是一个非常复杂的组件，算法很多优化的策略也有很多，它会通过自己的优化策略选择出优化器认为最优的一个执行计划，优化策略大致可以分为两种：**静态优化、动态优化**。

**静态优化**：可以直接对解析树进行分析、优化。优化器可以通过代数变换将`where`条件转换为另外一种等价形态，这个转换不依赖条件的具体数值，即使`where`条件中的数值发生改变，静态优化也仍然有效。可以理解为”**编译时优化**“。

**动态优化**：它与查询的上下文相关，影响动态优化的因素有很多，比如索引对应的数量行数、where条件中的值等等，这些都会让优化器在执行SQL的时候重新进行优化。可以理解为”**运行时优化**“。

所以优化器对SQL进行优化的时候是选择静态优化还是动态优化取决于SQL语句，静态优化只需要做一次，而动态优化在每次执行的时候都需要重新评估。

### 执行器

分析器将我们写的SQL语句解析成了语法树、优化器将语法树转成了它认为最优的一条执行计划，这个时候就需要真正的去读数据了，执行器就是那个执行者，前面的工作都是准备，分析器、优化器让MySQL知道了你要干什么、通过什么方式最简单有效，执行器就是拿着这些准备好的方案实施。

执行器在执行SQL之前还需要做一件事情：**权限验证**，执行器根据库名、表名、操作类型等等查看当前用户是否具备权限操作，如果发现当前用户并不具备此权限，那么直接终端执行，直接结束。

权限验证通过之后，执行器就会打开进入存储引擎，打开数据表进行数据读取，执行器也是通过存储引擎提供的API进行的操作。MySQL会重复执行计划中的每个操作，直到执行器查询完所有的数据为止，执行器没有分析器、优化器那么的复杂，它的主要功能就是执行。

### 存储引擎

MySQL中的数据用各种不同的技术存储在文件（或者内存）中。这些技术中的每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力。通过选择不同的技术，你能够获得额外的速度或者功能，从而改善你的应用的整体功能。

我们常用的存储引擎一般有一下几种：

> MyISAM：拥有较高的插入，查询速度，但不支持事务
> InnoDB：支持事务的存储引擎，mysql5.5以后将它设置为默认存储引擎。
> BDB：事务型数据库的另一种选择，支持COMMIT和ROLLBACK等其他事务特性
> Memory：基于内存的存储引擎，将所有的数据都置于内存中，查询、插入、删除效率极高，是一种空间换时间的思想，不过服务重启会导致数据丢失。
> Merge：将一部分的MyISAM表联合成的一个整体，适用于大数据存储。

存储引擎五花八门，所以MySQL将它设置成了插拔式结构，提高了MySQL的整体灵活性。

// TODO列举回表的情况 面试会经常问到

未发生索引覆盖，查询时查询条件不是主键

# WHERE 与 HAVING

`WHERE`与`HAVING`的根本区别在于：

- `WHERE`子句在`GROUP BY`分组和聚合函数**之前**对数据行进行过滤；
- `HAVING`子句对`GROUP BY`分组和聚合函数**之后**的数据行进行过滤。
- 聚合语句(sum,min,max,avg,count)要比having子句优先执行，所有having后面可以使用聚合函数。而where子句在查询过程中执行优先级别优先于聚合语句(sum,min,max,avg,count)，所有where条件中不能使用聚合函数。

因此，`WHERE`子句中不能使用聚合函数。例如，以下语句将会返回错误：

```sql
-- 查找人数大于 5 的部门
select dept_id, count(*)
from employee
where count(*) > 5
group by dept_id;
```

由于在执行`WHERE`子句时，还没有计算聚合函数 count(*)，所以无法使用。正确的方法是使用HAVING对聚合之后的结果进行过滤：

```sql
-- 查找人数大于 5 的部门
select dept_id, count(*)
from employee
group by dept_id
having count(*) > 5;
dept_id|count(*)|
-------|--------|
      4|       9|
      5|       8|
```

另一方面，`HAVING`子句中不能使用除了分组字段和聚合函数之外的其他字段。例如，以下语句将会返回错误：

```sql
-- 统计每个部门月薪大于等于 30000 的员工人数
select dept_id, count(*)
from employee
group by dept_id
having salary >= 30000;
```

因为经过`GROUP BY`分组和聚合函数之后，不再存在 salary 字段，`HAVING`子句中只能使用分组字段或者聚合函数。

> SQLite 虽然允许`HAVING`子句中出现其他字段，但是得到的结果不正确。

从性能的角度来说，`HAVING`子句中如果使用了分组字段作为过滤条件，应该替换成`WHERE`子句；因为`WHERE`可以在执行分组操作和计算聚合函数之前过滤掉不需要的数据，性能会更好。下面示例中的语句 1 应该替换成语句 2：

```sql
-- 语句 1
select dept_id, count(*)
from employee
group by dept_id
having dept_id = 1;

-- 语句 2
select dept_id, count(*)
from employee
where dept_id = 1
group by dept_id;
```

当然，`WHERE`和`HAVING`可以组合在一起使用。例如：

```sql
select dept_id, count(*)
from employee
where salary > 10000
group by dept_id
having count(*) > 1;
dept_id|count(*)|
-------|--------|
      1|       3|
```

该语句返回了月薪大于 10000 的员工人数大于 1 的部门；`WHERE`用于过滤月薪大于 10000 的员工；`HAVING`用于过滤员工数量大于 1 的部门。



# 隔离级别

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202211051835057.png" alt="img" style="zoom:67%;" />

1. **Read Uncommitted 读未提交**   --  脏读

   > 如果一个事务已经开始写数据，则另外一个事务不允许同时进行写操作，但允许其他事务读此行数据，该隔离级别可以通过“排他写锁”，但是 <font color='Chestnut Red'>**不排斥读线程实现。这样就避免了更新丢失，却可能出现脏读，也就是说事务B读取到了事务A未提交的数据**</font>

2. **Read Committed 读提交**   --  不可重复读

   > `Sql Server ,Oracle` 的默认隔离级别
   >
   > 如果是一个读事务，则允许其他事务读写，如果是写事务将会禁止其他事务访问该行数据，该 <font color='Chestnut Red'>**隔离级别避免了脏读，但是可能出现不可重复读**</font>。事务A事先读取了数据，事务B紧接着更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。
   >
   > <font color='Chestnut Red'>**解决了更新丢失和脏读问题，会造成不可重复读。**</font>

3. **Repeatable Read(可重复读取)**   --  幻读

   > `MySQL`的默认隔离级别
   >
   > 可重复读取是指在一个事务内，多次读同一个数据，在这个事务还没结束时，其他事务不能访问该数据(包括了读写)，这样就可以在同一个事务内两次读到的数据是一样的，因此称为是可重复读隔离级别，读取数据的事务将会禁止写事务(但允许读事务)，写事务则禁止任何其他事务(包括了读写)，这样避免了不可重复读和脏读，但是有时可能会出现幻读。(读取数据的事务)可以通过“共享读锁”和“排他写锁”实现。
   >
   > <font color='Chestnut Red'>**解决了更新丢失、脏读、不可重复读、但是还会出现幻读**</font>

4. **Serializable(可序化)**

   有如下特点：

   > 1. <font color='RedOrange'>事务串行执行</font> ：此隔离级别下，每个事务都是串行执行的，一个事务必须等待另一个事务完成后才能开始执行。这确保了事务之间没有并发执行，从而避免了并发带来的一致性问题。
   > 2. <font color='RedOrange'>锁定范围广</font>：为了保证串行化执行，数据库系统需要对更多的资源进行锁定，包括行锁、表锁或其他更高级别的锁。这可能导致更多的阻塞和竞争，降低系统的并发性能。
   > 3. <font color='RedOrange'>一致性</font>：串行化级别提供了最高的一致性，确保事务的执行顺序与它们提交的顺序相同。这样可以避免一些其他隔离级别下可能出现的一致性问题，如幻读、不可重复读等。
   > 4. <font color='RedOrange'>并发性能较低</font>：由于事务需要按照串行的方式执行，串行化隔离级别通常会导致系统的并发性能较低，因为事务需要等待其他事务的释放锁才能进行。
   >
   > 可以避免脏读、不可重复读、幻读
   >
   > <font color='Chestnut Red'>**解决了更新丢失、脏读、不可重复读、幻读(虚读)**</font>

## 概念及区别

1. 脏读

   >  在不同事务下，当前事务可以读取到另外事务未提交的数据

2. 不可重复读 -- <font color='Chestnut Red'>**侧重 修改 操作**</font>

   > 在一个事务内多次读取同一数据集合，在这个事务还没结束时。另外一个事务也访问该同一数据集合并做了 DML 操作，这样一个事务内两次读取到的数据可能是不一样的（InnoDB 通过使用 Next-Key Lock来避免不可重复读的问题）

3. 幻读 -- <font color='Chestnut Red'>**侧重 添加 和 删除 操作**</font>

   > 事务B前后两次读取同一个范围的数据，在事务B两次读取的过程中事务A新增了数据，导致事务B后一次读取到前一次查询没有看到的行。
   >
   > <font color='Magenta'>幻读和不可重复读有些类似，但是幻读强调的是集合的增减，而不是单条数据的更新</font>。

4. 丢失更新

   一个事务的更新操作被另外一个事务的更新操作所覆盖，从而导致数据不一致

## RR 和 RC 的区别

想要搞清楚这个问题，我们需要先弄清楚 RR 和 RC 的区别，分析下各自的优缺点。

<font color='RedOrange'>一致性读</font>

> 一致性读，又称为快照读。快照即当前行数据之前的历史版本。快照读就是使用快照信息显示基于某个时间点的查询结果，而不考虑与此同时运行的其他事务所执行的更改。
>
> 在MySQL 中，只有READ COMMITTED 和 REPEATABLE READ这两种事务隔离级别才会使用一致性读。
>
> 在 RC 中，每次读取都会重新生成一个快照，总是读取行的最新版本。
>
> 在 RR 中，快照会在事务中第一次SELECT语句执行时生成，只有在本事务中对数据进行更改才会更新快照。
>
> 在数据库的 <font color='Magenta'>RC 这种隔离级别中，还支持"半一致读"</font> ，一条update语句，如果 where 条件匹配到的记录已经加锁，那么InnoDB会返回记录最近提交的版本，由MySQL上层判断此是否需要真的加锁。

<font color='RedOrange'>锁机制</font>

> 数据库的锁，在不同的事务隔离级别下采用了不同的机制。在 MySQL 中，有三种类型的锁，分别是Record Lock、Gap Lock和 Next-Key Lock。
>
> 1. Record Lock表示记录锁，锁的是索引记录。
> 2. Gap Lock是间隙锁，锁的是索引记录之间的间隙。
> 3. Next-Key Lock是Record Lock和Gap Lock的组合，同时锁索引记录和间隙。他的范围是左开右闭的。
>
> 在 RC 中，只会对索引增加Record Lock，不会添加Gap Lock和Next-Key Lock。
>
> 在 RR 中，为了解决幻读的问题，在支持Record Lock的同时，还支持Gap Lock和Next-Key Lock；

<font color='RedOrange'>主从同步</font>

> 在数据主从同步时，不同格式的 binlog 也对事务隔离级别有要求。
>
> MySQL的binlog主要支持三种格式，分别是statement、row以及mixed，但是，RC 隔离级别只支持row格式的binlog。如果指定了mixed作为 binlog 格式，那么如果使用RC，服务器会自动使用基于row 格式的日志记录。
>
> 而 RR 的隔离级别同时支持statement、row以及mixed三种。

## 为何选 RC

1. 提升并发

   > 首先，RC 在加锁的过程中，是不需要添加Gap Lock和 Next-Key Lock 的，只对要修改的记录添加行级锁就行了。
   >
   > 这就使得并发度要比 RR 高很多。
   >
   > 另外，因为 RC 还支持"半一致读"，可以大大的减少了更新语句时行锁的冲突；对于不满足更新条件的记录，可以提前释放锁，提升并发度。

2. 减少死锁

   > 因为RR这种事务隔离级别会增加Gap Lock和 Next-Key Lock，这就使得锁的粒度变大，那么就会使得死锁的概率增大。
   >
   > 死锁：一个事务锁住了表A，然后又访问表B；另一个事务锁住了表B，然后企图访问表A；这时就会互相等待对方释放锁，就导致了死锁。

## 隔离级别与锁的关系

<font color='RedOrange'>Read Uncommitted</font> 级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 

<font color='RedOrange'>Read Committed</font> 级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁

<font color='RedOrange'>Repeatable Read</font> 级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 

<font color='RedOrange'>SERIALIZABLE</font> 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。

# MVCC

同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC），在某个事务对其进行操作的时候，需要查看这一条记录的隐藏列事务版本id，比对事务id并根据事物隔离级别去判断读取哪个版本的数据。**MVCC最大的优势：读不加锁，读写不冲突。**在读多写少的场景下增加了系统的并发性能。

`MySQL`的`InnoDB`引擎中，**读已提交**和**可重复读**都是基于MVCC实现的，区别在于**读已提交**隔离级别下，每次查询语句执行时，均会基于当前数据库的最新状态生成快照，而**可重复读**隔离级别下，只会在第一次查询时基于当前数据库的最新状态生成快照。

<span style="background:#f9eda6;">`MVCC`技术本质是依靠`undo log`和`Read View 快照`机制来实现的</span>。`undo log`记录了某条数据变更前的旧数据。在`MySQL`的`InnoDB`引擎中，每一条数据除了原本的字段外，还有三个隐藏字段，

​	`TRX_ID`：最后一次变更（增删改）本行数据的事务的`id`，事务每次开启前，都会从数据库获得一个**自增**长的事务ID，可以从事务ID判断事务的执行先后顺序。。

​	`roll_pointer`：指向本行数据的`undo log`的指针。

​	`ROW_ID`：（非必须）如果表中没有主键和非NULL唯一键时，则会有第三个隐藏的主键列**row_id**，用来生成默认的聚集索引。采用聚集索引的方式可以提升数据的查找效率。

变更数据时多个版本之间通过`DB_ROLL_PTR`相连并构成了一个**undo log回滚链**，以**undo log回滚链**为基础可以保证事务的**原子性**（事务回滚）和**隔离性（MVCC）**。

### Read View

- <font color='Apricot'>Read View是什么</font> ：它就是事务执行SQL语句时，产生的读视图。实际上在 InnoDB 中，每个SQL语句执行前都会得到一个Read View。
- <font color='Apricot'>Read View有什么用</font>： 它主要用来判断当前事务可看到哪个版本的数据，只有当前事务修改的未 commit 版本和所有已提交事务版本允许被访问

快照(`Read View`)只会在**读已提交**和**可重复读**隔离级别下执行查询语句时生成；这里的**生成快照**可以理解为**决定可见的数据的范围**，快照生成完毕，即当前事务可见的数据的范围也确定了。事务生成的**快照**就是那一刻事务所能看到的数据库的状态，后续无论其它事务如何对数据库进行操作，生成**快照**的事务只能看到快照所展示的数据库的状态；

> 在<span style="background:#f9eda6;">读已提交(RC)</span>隔离级别下，每次执行查询语句时均会生成快照;
>
> 在<span style="background:#f9eda6;">可重复读(RR)</span>隔离级别下，只会在第一次执行查询语句时生成快照。

<span style="background:#f9eda6;">`Read View`的组成内容：</span>

​	`m_ids`：定义`Read View`那一刻的数据库中所有未提交事务的`id`数组。

​	`min_trx_id`：`m_ids`中的最小值。

​	`max_trx_id`：表示生成`ReadView`时，系统中应该分配给下一个事务的`id`值。

​	`creator_trx_id`：创建当前`Read View`的事务的`id`。

> max_trx_id并不是m_ids中的最大值，事务id是递增分配的。比如现在有事务id为1，2，3这三个事务，之后事务id为3的事务提交了，当有一个新的事务生成ReadView时，m_ids的值就包括1和2，min_trx_id的值就是1，max_trx_id的值就是4

由于事务`id`是严格递增的，所以`Read View`可以用下图进行示意。

![image-20241117012632856](https://gitee.com/qc_faith/picture/raw/master/image/202411170126974.png)

事务生成快照时，快照中包含哪些数据（哪些数据当前事务可见），是基于**Read View**和**undo log回滚链**决定的，对于每条数据，会先从其最新版本进行判断，如果判断不可见，则根据**undo log回滚链**找到旧版本并继续判断，如果某条数据所有版本都被判断为不可见，则说明这条数据对当前事务不可见，快照中不会包含这条数据的任何版本。判断规则如下所示。

- 如果某版本的数据的<font color='RedOrange'>DB_TRX_ID < min_trx_id</font>，表明生成该版本的事务在生成`Read View`前，已经提交(因为事务ID是递增的)，故这个版本的数据对当前事务**可见**；
- 如果某版本的数据的<font color='RedOrange'>DB_TRX_ID >= max_trx_id</font>，说明最后修改这个版本的数据的事务在快照生成时还未创建，故这个版本的数据对当前事务**不可见**；
- 如果 <font color='RedOrange'>min_trx_id =< trx_id < max_trx_id</font>，分3种情况讨论：
  - （1）如果`m_ids`包含`trx_id`，则代表`Read View`生成时刻，这个事务还未提交，但是如果数据的`trx_id`等于`creator_trx_id`的话，表明数据是自己生成的，因此是**可见**的。
  - （2）如果`m_ids`包含`trx_id`，并且`trx_id`不等于`creator_trx_id`，则Read View生成时，事务未提交，并且不是自己生产的，所以当前事务**不可见**；
  - （3）如果`m_ids`不包含`trx_id`，则说明你这个事务在`Read View`生成之前就已经提交了，修改的结果，当前事务是**可见**的。

<font color='Apricot'>**总结就是只有当前事务修改的未 commit 版本和所有已提交事务版本允许被访问**</font>。

# 主从同步

## 优势：

1. 可以实时灾备，用于故障切换；
2. 读写分离，提供查询服务，实现负载均衡；

## 实现原理

<img src="https://gitee.com/qc_faith/picture/raw/master/image/202401041230418.png" alt="image-20240104122926268" style="zoom: 33%;" />

1. 主服务器MySQL服务将所有的写操作记录在 binlog 日志中，并生成 log dump 线程，将 binlog 日志传给从服务器MySQL服务的 I/O 线程。
2. 从服务器MySQL服务生成两个线程，一个是 I/O 线程，另一个是 SQL 线程。
3. 从库 I/O 线程去请求主库的 binlog 日志，并将 binlog 日志中的文件写入 relaylog（中继日志）中。
4. 从库的 SQL 线程会读取 relaylog 中的内容，并解析成具体的操作，来实现主从的操作一致，达到最终两个数据库数据一致的目的。

## 主从同步策略

<font color='RedOrange'>全同步复制</font>

> 客户端执行SQL时候，必须等到**所有从库**和**主库**都提交成功之后才能返回响应给客户端
>
> - 优点：可以保证数据不丢失
> - 缺点：效率很低，随着机器的增加，执行的效率会越来越低，因为要同步的机器有很多，只有等所有机器都提交之后才能返回响应给客户端

<font color='RedOrange'>异步复制</font>

> 客户端执行SQL之后，只会通知线程将binlog日志发送给从库，主库直接提交，不会等从库的响应，效率很高
>
> - 优点：效率高
> - 缺点：数据会丢失，如果同步的时候，主库提交了，但是由于一些原因导致从库并没有获取到主库发送过来的binlog日志，这时候主库挂了，那么这条数据在从库就找不到了
> - 适用场景：数据安全性不高，效率很高的场景

<font color='RedOrange'>半同步复制</font>

> 客户端执行SQL之后，至少要有一个从库同步成功才能返回响应给客户端，也就是说即使现在主库挂了，那么从库还是有这条数据
>
> 在全同步与异步取了折中的方案，<font color='Peach'>推荐此方案，可保证数据一致性</font>

<font color='RedOrange'>GTID复制</font>

>在MySql5.6之后提供了GTID复制。 GTID全程是 Global Trancation ID，是由MySql机器的唯一id+TID组成的，TID是当前MySql实例已经提交的事务数量，是逐渐递增的。
>
>Matser在更新数据之前就会生成一个GTID，一同记录到binlog中，再将binlog同步给Slave之后，Slave会先拿到GTID，然后判断这个GTID是否已经被处理过，如果被处理过就忽略，如果没有就执行
>
>优点：
>
>- 不需要再去找position进行同步了
>- 比传统复制简单

# 读写分离

读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。

因为主从复制要求slave不能写只能读 （如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时需要手动同步一下slave）。 

<font color='RedOrange'>基于*MySQL proxy* 代理的方式</font> 

><font color='Tasma'>实现方式</font>：在应用和数据库之间增加代理层，代理层接收应用对数据库的请求，根据不同请求类型转发到不同的实例，在实现读写分离的同时可以实现负载均衡。
>
><font color='Tasma'>优点</font>：直接实现读写分离和负载均衡，不用修改代码，master 和 slave 用一样的帐号，mysql 官方不建议实际生产中使用
>
><font color='Tasma'>缺点</font>：低性能， 不支持事务

<font color='RedOrange'>基于*应用内路由* 的方式</font>

><font color='Tasma'>实现方式</font>：基于spring的aop实现: 使用AbstractRoutingDataSource + aop + annotation 在dao层决定数据源。如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过 mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve 库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题，也就是不支持事务，所以我们还需要重写一下 DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。

<font color='RedOrange'>基于*MySQL* 驱动的方式</font>

> <font color='Tasma'>实现方案</font>：使用mysql驱动`Connector/J`的可以实现读写分离。即在jdbc的url中配置为如下的形示：`jdbc:mysql:replication://master,slave1,slave2,slave3/test`
>
> Java程序通过在连接MySQL的 jdbc 中配置主库与从库等地址，jdbc会自动将读请求发送给从库，将写请求发送给主库，此外，mysql的jdbc驱动还能够实现多个从库的负载均衡。

<font color='RedOrange'>基于*sharding-jdbc* 的方式</font>

> sharding-sphere是强大的读写分离、分表分库中间件，sharding-jdbc是sharding-sphere的核心模块。集成 sharding-jdbc 到项目中即可

# 执行计划

## 用法与作用

`explain + sql` ——>  可以显示优化器关于`SQL`执行的信息,`MySQL`解释了它将如何处理该语句，主要包含：

- 表的加载顺序
- `sql` 的查询类型
- 可能用到的索引，实际用到的索引
- 表与表之间的引用关系
- 一个表中有多少行被优化器查询 .....

## 包含的信息

![image-20220106114125229](https://gitee.com/qc_faith/picture/raw/master/image/image-20220106114125229.png)

执行计划包含这12个字段： `id`、`select_type`、`table`、`partitions`、`type`、`possible_keys`、`key`、`key_len`、`ref`、`rows`、`filtered`、`Extra` 。

### 1. ID

`id：` ：表示查询中执行select子句或者操作表的顺序，**`id`的值越大，代表优先级越高，越先执行**。 `id`大致会出现 3种情况：

`id`相同：具有同样的优先级，执行顺序由上而下，具体顺序由优化器决定。

`id`不同：如果 `SQL` 中存在子查询， `id`的序号会递增，`id`值越大优先级越高，越先被执行 。

`id`为`null`：最后执行

> 1. 有多少 `SELECT` 关键字，就有多少组 `id`
>
> 2. 对于**连接查询**来说，一个`SELECT`关键字后边的`FROM`子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的`id`值都是相同的
>
> 3. 对于包含**子查询**的查询语句来说，就可能涉及多个`SELECT`关键字，所以在包含子查询的查询语句的执行计划中，每个`SELECT`关键字都会对应一个唯一的`id`值
>
> 4. 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询，查看执行计划可以知道查询优化器是否对某个包含子查询的语句进行了重写
>
> 5. `id`值是`NULL`，而且`table`列名称也不是真实的表名或者表别名
>
>    `UNION`子句的作用：把多个查询的结果集合并起来并对结果集中的记录进行去重。`MySQL`使用内部的临时表对结果集去重。在内部创建了一个名为`<union1, 2>`的临时表对`id`为`1`的查询和`id`为`2`的查询的结果集合并起来并去重，所以，**id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。**
>
>    包含UNION ALL子句的查询的执行计划中，没有`id`为`NULL`的记录，UNION ALL它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，不需要为最终的结果集进行去重，所以也就不需要使用临时表。

![image-20220106152946227](https://gitee.com/qc_faith/picture/raw/master/image/image-20220106152946227.png)

### 2. select_type

表示 `select` 查询的类型，主要是用于区分各种复杂的查询，例如：`普通查询`、`联合查询`、`子查询`等。

①、SIMPLE

查询语句中不包含`UNION`或者子查询的查询都算作是`SIMPLE`类型（连接查询也算是`SIMPLE`类型）

②、PRIMARY

对于包含`UNION`、`UNION ALL`或者子查询的大查询来说，它是由几个小查询组成的，最外层 `SELECT `被标记为`PRIMARY`。

③、UNION

对于包含`UNION`、`UNION ALL`或者子查询的大查询来说，最外层 `SELECT `被标记为`PRIMARY`，其余都是`UNION`

④、UNION RESULT

`UNION RESULT`：从`union`的临时表中读取数据的 `select `被标记成 `union result` 。

⑤、SUBQUERY

如果包含子查询的查询语句不能够转为对应的`semi-join`的形式，并且该子查询是**不相关子查询**，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个`SELECT`关键

字代表的那个查询的`select_type`就是`SUBQUERY`

`select_type`为`SUBQUERY`的子查询由于会被物化，所以只需要执行一遍

> **物化：将不相关子查询的结果集写入一个临时表里而非当作外层查询的参数** 

⑥、DEPENDENT SUBQUERY

如果包含子查询的查询语句不能够转为对应的`semi-join`的形式，并且该子查询是**相关子查询**，则该子查询的第一个`SELECT`关键字代表的那个查询的`select_type`就是`DEPENDENT SUBQUERY`

`select_type`为`DEPENDENT SUBQUERY`的查询可能会被执行多次。

⑦、DEPENDENT UNION

在包含`UNION`或者`UNION ALL`的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的`select_type`的值就是`DEPENDENT UNION`。

⑧、DERIVED

对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的`select_type`就是`DERIVED`

⑨、MATERIALIZED

当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的`select_type`属性就是`MATERIALIZED`

### 3. table

查询的表名，可能是真实存在的表，有别名时显示别名，也可能为临时表

### 4. partitions

查询时匹配到的分区信息，对于非分区表值为`NULL`，当查询的是分区表时，`partitions`显示分区表命中的分区情况

### 5. type

执行计划的一条记录就代表着`MySQL`对某个表执行查询时的访问方法，其中的`type`列就表明了这个访问方法是个啥

性能从好到坏依次是：`system`  > `const` > `eq_ref` > `ref`  > `ref_or_null` > `index_merge` > `unique_subquery` > `index_subquery` > `range` > `index` > `ALL`

一般来说，得保证查询达到 `range` 级别，最好达到 `ref`

**`system`**： 当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如`MyISAM`、`Memory`，那么对该表的访问方法就是system。

**`const`**：表示查询时命中 `primary key` 主键或者 `unique` 唯一索引，或者被连接的部分是一个常量(`const`)值。这类查询速度非常快

**`eq_ref`**：在连接查询时，如果被驱动表是通过主键或者唯一索引列等值匹配的方式进行访问的（如果该主键或者唯一索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是`eq_ref`

**`ref_or_null`**：当对普通二级索引进行等值匹配查询，该索引列的值也可以是`NULL`值时，那么对该表的访问方法就可能是`ref_or_null`

**`index_merge`**：一般情况下对于某个表的查询只能使用到一个索引，单表访问方法时在某些场景下可以使用`Intersection、Union、Sort-Union`这三种索引合并的方式来执行查询

**`unique_subquery`**：类似于两表连接中被驱动表的`eq_ref`访问方法，`unique_subquery`是针对在一些包含`IN`子查询的查询语句中，如果查询优化器决定将`IN`子查询转换为`EXISTS`子查询，而且子查询可以使用到主键进行等值匹配

的话，那么该子查询执行计划的`type`列的值就是`unique_subquery`

**`index_subquery`**：`index_subquery`与`unique_subquery`类似，只不过访问子查询中的表时使用的是普通的索引

**`range`**：如果使用索引获取某些范围区间的记录，那么就可能使用到`range`访问方法

**`index`**：当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是`index`

**`ALL`**：全表扫描

> 除了`All`，其余的访问方法都能用到索引

### 6. possible_keys和key

`possible_keys`列表示在查询语句中，对某个表执行单表查询时可能用到的索引，`key`列表示实际用到的索引，查询优化器会计算使用不同索引的成本

`possible_keys`列中的值不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，尽量删除用不到的索引。

### 7. key_len

`key_len`列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的：

- 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是`VARCHAR(100)`，使用的字符集是`utf8`，那么该列实际占用的最大存储空间就是`100 × 3 = 300`个字节。

- 如果该索引列可以存储`NULL`值，则`key_len`比不可以存储`NULL`值时多1个字节。

- 对于变长字段来说，都会有`2`个字节的空间来存储该变长列的实际长度。

### 8. ref

当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是`const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery`其中之一时，`ref`列展示的就是谁在与索引列作等值匹配，比如只是一个常数或者是

某个列

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106173906914.png" alt="image-20220106173906914" style="zoom:67%;" />

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106174041253.png" alt="image-20220106174041253" style="zoom:67%;" />

### 9. rows

如果查询优化器决定使用**全表扫描**的方式对某个表执行查询时，执行计划的`rows`列就代表预计需要扫描的**行数**，如果使用**索引**来执行查询时，执行计划的`rows`列就代表预计扫描的**索引记录行数**

### 10. filtered

如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。

如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。

<img src="https://gitee.com/qc_faith/picture/raw/master/image/image-20220106175245267.png" alt="image-20220106175245267" style="zoom:67%;" />

### 11. Extra

此列用来说明一些额外信息

**Using index**

当查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。

**Using index condition**

有些搜索条件中虽然出现了索引列，但却不能使用到索引

**Using where**

当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息

当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。

# DELETE 、TRUNCATE、DROP

| 区别点          | drop                                                       | truncate                                                 | delete                                                       |
| --------------- | ---------------------------------------------------------- | -------------------------------------------------------- | ------------------------------------------------------------ |
| 执行速度        | 快                                                         | 较快                                                     | 慢                                                           |
| 命令分类        | DDL（数据定义语言）                                        | DDL（数据定义语言）                                      | DML（数据操作语言）                                          |
| 删除对象        | 删除整张表和表结构，以及表的索引、约束和触发器。并释放空间 | 只删除表数据，表的结构、索引、约束等会被保留。并释放空间 | 只删除表的全部或部分数据，表结构、索引、约束等会被保留。不释放空间 |
| 删除条件(where) | 不能用                                                     | 不能用                                                   | 可使用                                                       |
| 回滚            | 不可回滚                                                   | 不可回滚                                                 | 可回滚                                                       |
| 自增初始值      | -                                                          | 重置                                                     | 不重置                                                       |

DELETE：

> 用于删除表中的数据行，但保留表的结构。在执行 DELETE 语句时，数据库会逐行删除满足条件的数据，并且可以搭配事务使用，可以回滚。
> 删除的数据行会被放入事务日志中，因此可以通过回滚操作恢复被删除的数据。
> DELETE 操作是逐行进行的，因此对于大表或者删除大量数据时会比较慢，同时会产生大量的日志。

TRUNCATE：

> 用于删除表中的所有数据行，并且释放表所占用的存储空间。TRUNCATE 不是一个标准的 SQL 操作，而是由数据库厂商实现的。
> TRUNCATE 操作是以表为单位进行的，而不是逐行进行的，因此在删除大量数据时速度较快。操作不能回滚，因为它是直接删除表中的数据，而不是逐行进行的。

总的来说，DELETE 适合删除少量数据或者需要回滚的场景，而 TRUNCATE 适合删除大量数据或者清空表的场景。在内存回收方面，TRUNCATE 操作会更快，因为它是以表为单位进行删除的，而且不会产生大量的日志。